#!/usr/bin/env python3

#
# Run the test, compare results against the benchmark
#

# requires: petsc
# requires: all_tests
# cores: 4

# Variables to compare
from __future__ import print_function
from builtins import str

vars = [
    ("max_error1", 2.0e-4),
    ("max_error2", 2.0e-4),
    ("max_error3", 2.0e-4),
    ("max_error4", 1.0e-5),
    ("max_error5", 2.0e-4),
    ("max_error6", 2.0e-5),
    ("max_error7", 2.0e-4),
    ("max_error8", 2.0e-5),
]
# tol = 1e-4                  # Absolute (?) tolerance

from boututils.run_wrapper import build_and_log, shell, launch_safe
from boutdata.collect import collect

# import numpy as np
from sys import stdout, exit


build_and_log("PETSc Laplacian inversion test")

print("Running PETSc Laplacian inversion test")
success = True

for nproc in [1, 2, 4]:
    #  nxpe = 1
    #  if nproc > 2:
    #    nxpe = 2

    cmd = "./test_petsc_laplace"

    shell("rm data/BOUT.dmp.*.nc")

    print("   %d processors...." % nproc)
    s, out = launch_safe(cmd, nproc=nproc, pipe=True, verbose=True)
    f = open("run.log." + str(nproc), "w")
    f.write(out)
    f.close()

    # Collect output data
    for varname, tol in vars:
        stdout.write("      Checking " + varname + " ... ")
        error = collect(varname, path="data", info=False)
        if error <= 0:
            print("Convergence error")
            success = False
        elif error > tol:
            print("Fail, maximum error is = " + str(error))
            success = False
        else:
            print("Pass")

if success:
    print(" => All PETSc Laplacian inversion tests passed")
    exit(0)
else:
    print(" => Some failed tests")
    exit(1)
