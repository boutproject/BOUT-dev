#!/usr/bin/env python3

#requires: not travis
# 
# Run the test, compare results against the benchmark
#

# Variables to compare
from __future__ import print_function
try:
  from builtins import str
except:
  pass

from boututils.run_wrapper import build_and_log, launch
from boutdata.collect import collect
import numpy as np
from sys import stdout, exit

nproc = 1


build_and_log("Making stopCheck test")

checkVal=[True,False]
nstepExpect=[1,11]
dats = ["data","dataSecond"]
datStopFiles = ["BOUT.stop","otherStop.check"]
 
vars=["t_array"]

print("Running stopCheck test")
success = True

for j, dat in enumerate(dats):
  for i, check in enumerate(checkVal):
    cmd = "../../../bin/test_stopCheck"

    s, out = launch(cmd+" -d "+dat+" stopCheck="+str(check)+" stopCheckName="+datStopFiles[j], nproc=nproc, pipe=True)
    f = open("run.log."+dat+"."+str(check).lower(), "w")
    f.write(out)
    f.close()
    
    # Collect output data
    for v in vars:
      result = collect(v, path=dat, info=False)

      # Compare benchmark and output
      if result.shape[0] != nstepExpect[i]:
        print("Fail, wrong shape")
        print("Option is "+str(check)+"/"+dat+"/"+datStopFiles[j])
        print("shape is "+str(result.shape[0]))
        print("expecting "+str(nstepExpect[i]))
        success = False
        continue

if success:
  print(" => All checkStop tests passed")
  exit(0)
else:
  print(" => Some failed tests")
  exit(1)
