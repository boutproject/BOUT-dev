%% Users manual for C++ BOUT code

\documentclass[12pt]{article}
\usepackage[nofoot]{geometry}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{hyperref}

\usepackage{subfigure}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.95,0.95,0.95}
\lstset{
	backgroundcolor=\color{lbcolor},
        language=C++,
	keywordstyle=\bfseries\ttfamily\color[rgb]{0,0,1},
	identifierstyle=\ttfamily,
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
	showstringspaces=false,
	basicstyle=\small,
	numberstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	numbersep=10pt,
	tabsize=2,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	breakatwhitespace=false,
	aboveskip={0.5\baselineskip},
        columns=fixed,
        upquote=true,
        extendedchars=true,
        morekeywords={Field2D,Field3D,Vector2D,Vector3D,BoutReal,FieldGroup},
}

% Add an index
\usepackage{makeidx}
\makeindex

%% Modify margins
\addtolength{\oddsidemargin}{-.25in}
\addtolength{\evensidemargin}{-.25in}
\addtolength{\textwidth}{0.5in}
\addtolength{\textheight}{0.25in}
%% SET HEADERS AND FOOTERS

\pagestyle{fancy}
\fancyfoot{}
\renewcommand{\sectionmark}[1]{         % Lower case Section marker style
  \markright{\thesection.\ #1}}
\fancyhead[LE,RO]{\bfseries\thepage}    % Page number (boldface) in left on even
                                        % pages and right on odd pages 
\renewcommand{\headrulewidth}{0.3pt}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\bfile}[1]{\texttt{\bf 1}}

%% commands for boxes with important notes
\newlength{\notewidth}
\addtolength{\notewidth}{\textwidth}
\addtolength{\notewidth}{-3.\parindent}
\newcommand{\note}[1]{
\fbox{
\begin{minipage}{\notewidth}
{\bf NOTE}: #1
\end{minipage}
}}

\newcommand{\pow}{\ensuremath{\wedge} }
\newcommand{\poweq}{\ensuremath{\wedge =} }

\newcommand{\deriv}[2]{\ensuremath{\frac{\partial #1}{\partial #2}}}

\newcommand{\rbtsq}{\ensuremath{\left(RB_\theta\right)^2}}
\newcommand{\sbt}{\ensuremath{\sigma_{B\theta}}}
\newcommand{\apar}{\ensuremath{A_{||}}}
\newcommand{\Bthe}{\ensuremath{B_\theta}}
\newcommand{\Bzeta}{\ensuremath{B_\zeta}}
\newcommand{\hthe}{\ensuremath{h_\theta}}

\begin{document}

\title{BOUT++ Users Manual}
\author{B.Dudson, University of York}
\date{July 14, 2010}
\maketitle

\tableofcontents

\section{Introduction}

BOUT++ is a C++ framework for writing plasma fluid simulations with an arbitrary number of equations
in 3D curvilinear coordinates \cite{Dudson2009,dudson-2008-arxiv}. It has been developed from the original {\bf BOU}ndary {\bf T}urbulence
3D 2-fluid edge simulation code \cite{bout_manual,umansky-2008-bout,xu-2008} written by X.Xu and M.Umansky at LLNL.

Though designed to simulate tokamak edge plasmas, the methods used are very general and almost
any metric tensor can be specified, allowing the code to be used to simulate (for example) plasmas in 
slab, sheared slab, and cylindrical coordinates. The restrictions on the simulation domain
are that the equilibrium must be axisymmetric (in the z coordinate), and that the parallelisation
is done in the $x$ and $y$ (parallel to $\mathbf{B}$) directions.

The aim of BOUT++ is to automate the common tasks needed for simulation codes, and to separate the
complicated (and error-prone) details such as differential geometry, parallel communication, and
file input/output from the user-specified equations to be solved. Thus the equations being solved
are made clear, and can be easily changed with only minimal knowledge of the inner workings of the
code. As far as possible, this allows the user to concentrate on the physics, rather than worrying
about the numerics. This doesn't mean that users don't have to think about numerical methods, and so selecting differencing schemes and boundary conditions is discussed in this manual. The generality of the BOUT++ of course also comes with a limitation: although there is a large class of problems which can be tackled by this code, there are many more problems which require a more specialised solver and which BOUT++ will not be able to handle. Hopefully this manual will enable you to test whether BOUT++ is suitable for your problem as quickly and painlessly as possible.

This manual is written for the user who wants to run (or modify) existing plasma models, or 
specify a new problem (grid and equations) to be solved. In either case, it's assumed that
the user isn't all that interested in the details of the code.
For a more detailed descriptions of the code internals, see the developer
and reference quides.
After describing how to install BOUT++ (section~\ref{sec:install}) and run a few examples 
(section~\ref{sec:runexamples}, but described in detail in section~\ref{sec:examples}), increasingly sophisticated ways to modify the problem being solved are introduced.
The simplest way to modify a simulation case is by altering the input options, described in
section~\ref{sec:options}. Checking that the options are doing what you think they should be
by looking at the output logs is described in section~\ref{sec:running}, and an overview of the IDL analysis routines
for data post-processing and visualisation is given in section~\ref{sec:output}.
Generating new grid files, particularly for tokamak equilibria, is described in section~\ref{sec:gridgen}.

Up to this point, little programming experience has been assumed, but performing more drastic
alterations to the physics model requires modifying C++ code.
Section~\ref{sec:equations} describes how to write a new physics model specifying the equations to be solved,
using ideal MHD as an example. The remaining sections describe in more detail aspects of using BOUT++:
section~\ref{sec:diffops} describes the differential operators and methods available; section~\ref{sec:staggergrids}
covers the experimental staggered grid system.

\subsection{License and terms of use}

\begin{verbatim}
Copyright 2010 B.D.Dudson, S.Farley, M.V.Umansky, X.Q.Xu

BOUT++ is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

BOUT++ is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with BOUT++.  If not, see <http://www.gnu.org/licenses/>.

A copy of the LGPL license is in COPYING.LESSER. Since this is based
on (and refers to) the GPL, this is included in COPYING.
\end{verbatim}

BOUT++ is free software, but since it is a
scientific code we also ask that you show professional courtesy
when using this code:

\begin{enumerate}
\item Since you are benefiting from work on BOUT++, we ask that you
  submit any improvements you make to the code to us by emailing 
  Ben Dudson at bd512@york.ac.uk
\item If you use BOUT++ results in a paper or professional publication,
  we ask that you send your results to one of the BOUT++ authors
  first so that we can check them. It is understood that in most cases
  if one or more of the BOUT++ team are involved in preparing results
  then they should appear as co-authors.
\item Publications or figures made with the BOUT++ code should acknowledge the
  BOUT++ code by citing B.Dudson et. al. Comp.Phys.Comm 2009 \cite{Dudson2009}
  and/or other BOUT++ papers. See the file CITATION for details.
\end{enumerate}

\section{Starting}

BOUT++ is now hosted publicly on github (\url{http://github.com/bendudson/BOUT}), and includes
instructions on downloading and installing BOUT++ in wiki pages. This website also has current issues
and history of changes. To obtain a copy of the latest version, run \index{Github}
\begin{verbatim}
git clone git://github.com/bendudson/BOUT.git
\end{verbatim}
which will create a directory \texttt{BOUT} containing the code. To get the latest changes later,
go into the  \texttt{BOUT} directory and run
\begin{verbatim}
git pull
\end{verbatim}
For more details on using git to work with BOUT++, see the developer's manual.

Once you have got a copy of BOUT++, see the README files for brief descriptions of 
each file and directory. The core BOUT++ code is in the
\texttt{bout++} directory (with source in \texttt{bout++/src}), and
section~\ref{sec:install} describes how to compile this as a library which
can then be used by many different fluid models. Some example models and test
problems are in the \texttt{bout++/examples} directory and are described in
section~\ref{sec:examples}.

\subsection{Documentation}

Various sources of documentation are:
\begin{itemize}
\item Most directories in the BOUT++ distribution contain a README file. This should
describe briefly what the contents of the directory are and how to use them.
\item This user's manual, which goes through BOUT++ from a user's point of view
\item The developer's manual, which gives details of the internal working of the code.
\item The reference guide, which summarises functions, settings etc. Intended more for
quick reference rather than a guide.
\item Most of the code contains Doxygen comment tags (which are slowly getting better). 
Running doxygen (\url{www.doxygen.org}) on these files should therefore generate an HTML
reference. This is probably going to be the most up-to-date documentation.
\end{itemize}

\subsection{Installing libraries}
\label{sec:install}

BOUT++ uses several external libraries, and unless these are already installed,
you will need to install them yourself. 

BOUT++ can currently use two different file formats: Portable Data Binary
(PDB) which is part of PACT\footnote{\url{http://pact.llnl.gov}}, and
NetCDF-4\footnote{\url{http://www.unidata.ucar.edu/software/netcdf/}}.
PDB was developed at LLNL, was used in the UEDGE and BOUT codes, and 
was the original format used by BOUT++. NetCDF is a more widely used
format and so has many more tools for viewing and manipulating files.
In particular, the NetCDF-4 library can produce files in either
NetCDF3 ``classic'' format, which is backwards-compatible with NetCDF
libraries since 1992, or in the newer NetCDF4 format, which is based
on (and compatible with) HDF5. If you have both libraries installed then
BOUT++ can use both simultaneously, for example reading in grid files in PDB
format, but writing output data in NetCDF format. 

It should be possible to install and try out BOUT++ with a minimal number of
libraries, but BOUT++ can use the following libraries:
\begin{itemize}
  \item MPI (Message Passing Interface) and C++ compiler (e.g. \code{mpiCC} or \code{mpicxx}) {\bf [Required]}. Should work with most implementations, though mostly tested with MPICH and LAM.
  \item FFTW-3 (Fastest Fourier Transform  in the West) {\bf [Required]} \url{http://www.fftw.org/} 
  \item NetCDF-4 \url{http://www.unidata.ucar.edu/software/netcdf/} {\bf [Recommended]}. Does not need the HDF5 extensions.
  \item SUNDIALS \url{https://computation.llnl.gov/casc/sundials/main.html}. An old version (c.1998) of the SUNDIALS CVODE solver (then called PVODE) is included with BOUT++, but the latest CVODE solver is recommended.
  \item PACT \url{http://pact.llnl.gov/} for reading and writing Portable Data Binary (PDB) format files. This is mainly for backwards compatibility with BOUT and UEDGE, and NetCDF-4 is recommended.
  \item LAPACK for linear solvers
\end{itemize}

A bare minimum to get BOUT++ running is an MPI implementation (e.g. MPICH, OpenMPI, LAM), FFTW-3, and one (or both) of NetCDF or PACT. 

If you have adminstrator rights (root access) to your computer, then you could use a package manager
to install these libraries. Alternatively you could ask someone to install these for you. The Debian / Ubuntu
packages can be installed by running
\begin{verbatim}
$ sudo apt-get install mpich-bin libfftw3-dev libnetcdf-dev
\end{verbatim}

Most large machines / clusters (e.g. NERSC Franklin, HECToR etc) will usually have all these libraries
already installed (check the available modules, usually by running \texttt{modules avail}). If the 
libraries are not installed, then you will have to install them in your home directory.
First go to your home directory, and create a directory ``install'' which is going to be used for temporary
installation files, and a directory ``local'' where the libraries are going to be installed:
\begin{verbatim}
 ~/ $ mkdir install
 ~/ $ mkdir local
 ~/ $ mkdir local/bin
 ~/ $ mkdir local/lib
\end{verbatim}

Then add your local directories to environment variables. For BASH shells:
\begin{verbatim}
 ~/ $ export PATH=$HOME/local/bin:$PATH
 ~/ $ export LD_LIBRARY_PATH=$HOME/local/lib:$LD_LIBRARY_PATH
\end{verbatim}
or for csh type shells:
\begin{verbatim}
 ~/ $ setenv PATH $HOME/local/bin:$PATH
 ~/ $ setenv LD_LIBRARY_PATH $HOME/local/lib:$LD_LIBRARY_PATH
\end{verbatim}
These two lines should be added to the end of your login scripts (e.g. \texttt{~/.profile})

Then download the NetCDF, FFTW, SUNDIALS and (optionally) PACT libraries, and put them into ``install''. This
tutorial is with the following versions:
\begin{verbatim}
 ~/ $ ls install
fftw-3.2.1.tar.gz    pact-2.1.0.tar.gz
netcdf-4.0.1.tar.gz  sundials-2.4.0.tar.gz
\end{verbatim}

Before installing, make sure the correct version of \texttt{install} is being used by running
\begin{verbatim}
 ~/ $ which install
\end{verbatim}
This should point to a system directory like \texttt{/usr/bin/install}. Sometimes when IDL has been
installed, this points to the IDL install (e.g. something like \texttt{/usr/common/usg/idl/idl70/bin/install} on Franklin). A quick way to fix this is to create a link from your local bin to the system install:
\begin{verbatim}
 ~/ $ ln -s /usr/bin/install $HOME/local/bin/
\end{verbatim}
``which install'' should now print the install in your local bin directory.

Most of these libraries can be installed by unzipping the archive, configuring and making. For example, the NetCDF library can be built using:
\begin{verbatim}
~/install/ $ tar -xzvf netcdf-4.0.1.tar.gz
~/install/ $ cd netcdf-4.0.1
~/install/netcdf-4.0.1/ $ ./configure --prefix=$HOME/local
~/install/netcdf-4.0.1/ $ gmake
~/install/netcdf-4.0.1/ $ gmake install
\end{verbatim}

NOTES: 
\begin{itemize}
\item If you want to output to HDF5 then you need to first install
  the zlib and HDF5 libraries, and then compile NetCDF with HDF5 support.
\begin{verbatim}
~/install/netcdf-4.0.1/ $ ./configure --prefix=$HOME/local
\end{verbatim}
\item On Franklin, SUNDIALS needs to be compiled with ``--with-mpicc=cc'' to
  force it to build the parallel libraries
\end{itemize}

Details on installing PACT are given in Appendix~\ref{apx:pact}. NOTE: This
can be a giant hassle, and is only really needed
for backwards compatibility with BOUT. If you're not interested in using PDB files
you can skip this.

The SUNDIALS library needs to have MPI enabled, so after configuring, check the output and look for something like this:
\begin{verbatim}
MPI-C Settings
--------------

checking if using MPI-C script... yes
checking if absolute path to mpicc was given... no
checking for mpicc... none
configure: WARNING: cannot find MPI-C compiler
\end{verbatim}

If you see this warning, you need to tell CVODE which MPI compiler to use. This happens
on machines where the MPI compilers are given non-standard names. For IBM AIX machines,
use
\begin{verbatim}
$ ./configure --prefix=$HOME/local/ --with-mpicc=xlC
\end{verbatim}

\subsection{Installing BOUT++}
\label{sec:installbout}

Once you have the libraries installed, you can install BOUT++.
In the root directory of the BOUT++ distribution is a file \texttt{INSTALL} 
which will contain more details on installation, including troubleshooting.
For most systems however the installation should consist of running the
following commands in the BOUT++ root directory:
\begin{verbatim}
./configure
gmake
\end{verbatim}
This should detect the setting needed automatically. The main things it's
looking for are
\begin{itemize}
\item NetCDF installation. When NetCDF is installed, a script \texttt{nc-config}
should be put into somewhere on the path. If this is found then configure
should have all the settings it needs. If this isn't found then configure
will search for the NetCDF include and library files. To specify the location
of these files, use
\begin{verbatim}
    ./configure --with-netcdf=/path/to/netcdf/
\end{verbatim}
\item A PACT installation (\texttt{http://pact.llnl.gov}) for PDB file I/O. One of the places configure
  will look is in your home/local directory. If you do not want PDB support, configure with
\begin{verbatim}
    ./configure --with-pdb=no
\end{verbatim}
If you do this, you will need to enable NetCDF support (see below).
If you installed PACT into a non-standard location, you can specify this using
\begin{verbatim}
    ./configure PACT=/path/to/pact/
\end{verbatim}
\item MPI C and C++ compilers. These are usually called something like mpicc and mpiCC (or mpicxx), but if your compilers aren't recognised then set them using
\begin{verbatim}
     ./configure MPICC=<your C compiler> MPICXX=<your C++ compiler>
\end{verbatim}
\end{itemize}

When the compilation finishes the BOUT++ library should be in \texttt{lib/libbout++.a}.
If you have trouble compiling see the \texttt{INSTALL} file for more details.

To enable NetCDF support, you will need to install NetCDF version 4.0.1 or later. 
Note that although the NetCDF-4 library is used for the C++ interface, by default
BOUT++ writes the ``classic'' format, which can be read by any NetCDF installation
since 1994 (version 2.3). Because of this, you don't need to install zlib or HDF5
for BOUT++ NetCDF support to work.

To enable NetCDF support,
\begin{verbatim}
./configure --with-netcdf
\end{verbatim}
or
\begin{verbatim}
./configure --with-netcdf=/path/to/netcdf/
\end{verbatim}

NOTES:
\begin{itemize}
\item On LLNL's Grendel, mpicxx is broken. Use mpiCC instead by passing ``MPICXX=mpiCC'' to configure. Also need to specify this to NetCDF library by passing ``CXX=mpiCC'' to NetCDF configure.
\end{itemize}

\subsubsection{Solver library}
\label{sec:solverlibrary}

As of July 2010, BOUT++ can be compiled with several solver libraries, and the desired one
selected at runtime (thanks to factory magic by S.Farley).

The BOUT++ distribution used to include a 1998 version of CVODE by Scott D. Cohen and Alan C.
Hindmarsh, which was the default time integration solver. This is the easiest to install, and
can be downloaded and compiled automatically by using the option
\begin{verbatim}
   ./configure --download-pvode
\end{verbatim}
Whilst no serious bugs have been found in this code
(as far as I am aware), several features such as user-supplied preconditioners and constraints cannot
be used with this solver. Currently, BOUT++ also supports the SUNDIALS solvers CVODE and
IDA\footnote{available from \code{https://computation.llnl.gov/casc/sundials/main.html}}.

The modern SUNDIALS CVODE solver is essentially the same as the 1998 CVODE (with some tweaking,
re-arranging etc.), but this solver allows users to supply their own preconditioner 
and Jacobian functions. To use this solver, use
\begin{verbatim}
./configure --with-cvode
\end{verbatim}
or
\begin{verbatim}
./configure --with-cvode=/path/to/cvode/
\end{verbatim}
if your CVODE library is in a non-standard place.

The SUNDIALS IDA solver is a Differential-Algebraic Equation (DAE) solver, which evolves a system of the form
$\mathbf{f}\left(\mathbf{u}, \dot\mathbf{u}, t\right) = 0$. This allows algebraic constraints on variables
to be specified. If you want this functionality, compile in the IDA library using
\begin{verbatim}
./configure --with-ida
\end{verbatim}
or
\begin{verbatim}
./configure --with-ida=/path/to/ida/
\end{verbatim}

You can compile in several of these libraries, for example 
\begin{verbatim}
./configure --with-cvode --with-ida
\end{verbatim}
is valid. This will allow you to select at run-time which solver to use. See section~\ref{sec:timeoptions}
for more details on how to do this.

\subsubsection{PETSc}

BOUT++ can use PETSc for time-integration:
\begin{verbatim}
./configure --with-petsc
\end{verbatim}

This will allow use of a greater number of sophisticated time-integration
packages and preconditioning methods, and is under development.

\subsubsection{FFT library}

BOUT++ needs the the FFTW-3 (Fastest Fourier Transform in the West)
library\footnote{\code{http://www.fftw.org/}}. The configure script will
search for an installation, but if you've installed it in a strange place then
specify the path to the installation:
\begin{verbatim}
./configure --with-fftw=/path/to/fftw/
\end{verbatim}

\subsubsection{LAPACK}

BOUT++ comes with linear solvers for tridiagonal and band-diagonal systems,
but these are not particularly optimised and are in any case descended from Numerical Recipes code (hence NOT covered by LGPL license).

To replace these routines, BOUT++ can use the LAPACK library. This is
however written in FORTRAN 77, which can cause linking headaches.
To enable these routines use
\begin{verbatim}
./configure --with-lapack
\end{verbatim}
and to specify a non-standard path
\begin{verbatim}
./configure --with-lapack=/path/to/lapack
\end{verbatim}

\subsection{Running the examples}
\label{sec:runexamples}

To use the BOUT++ library, set an environment variable {\bf BOUT\_TOP} to point to the BOUT++ root
directory (the directory containing make.config and configure). Actually this is not strictly true since
you can compile the examples by just running
\begin{verbatim}
gmake examples
\end{verbatim}
in the BOUT++ root directory, but setting {\bf BOUT\_TOP} lets you compile examples in different
directories, and makes it easier to develop cases.

The \texttt{examples/} directory contains some test cases for a variety of fluid models. In many cases
there are two scripts for running and then an IDL script to plot the results:
\begin{verbatim}
./runcase.sh
idl runidl.pro
\end{verbatim}

Assuming these examples work (which they should), looking through the scripts and code
may give you an idea of how BOUT++ works. More information on setting up and running BOUT++ is given
in section~\ref{sec:running}, and details of analysing the results using IDL are given in
section~\ref{sec:output}.

\subsection{When things go wrong}

BOUT++ is still under development, and so occasionally you may be lucky
enough to discover a new bug. This is particularly likely if you're modifying
the physics module source code (see section~\ref{sec:equations}) when you
need a way to debug your code too.

\begin{itemize}
\item Check the end of each processor's log file (tail data/BOUT.log.*). When BOUT++ exits before
it should, what is printed to screen is just the output from processor 0.
If an error occurred on another processor then the error message will be written to it's log file instead.
\item By default when an error occurs a kind of stack trace is printed which shows which functions
were being run (most recent first). This should give a good indication of where an error occured.
If this stack isn't printed, make sure checking is set to level 2 or higher (./configure --with-checks=2)
\item If the error is a segmentation fault, you can try a debugger such as totalview
\item If the error is due to non-finite numbers, increase the checking level
(./configure --with-checks=3) to perform more checking of values
and (hopefully) find an error as soon as possible after it occurs.
\end{itemize}

\section{BOUT++ options}
\label{sec:options}
\index{options}

The inputs to BOUT++ are a binary grid file in NetCDF or PDB format,
and a text file with options. Generating input grids for tokamaks is
described in section~\ref{sec:gridgen}. The grid file describes the size
and topology of the X-Y domain, metric tensor components and usually
some initial profiles. The option file specifies the size of the domain
in the symmetric direction (Z), and controls how the equations are evolved
e.g. differencing schemes to use, and boundary conditions.
In most situations, the grid file will be used in many different simulations,
but the options may be changed frequently. 

The text input file \texttt{BOUT.inp} is always in a subdirectory called \texttt{data}
for all examples. The files include comments (starting with either ';' or '\#') and should be fairly
self-explanatory. The format is the same as a windows INI file, consisting
of \code{name = value} pairs. Comments are started
with a hash (\#) or semi-colon, which comments out the rest of the line. 
values can be:
\begin{itemize}
\item Integers
\item Real values
\item Booleans
\item Strings
\end{itemize}
Options are also divided into sections, which start with the section name in square brackets.
\begin{lstlisting}[language=bash,numbers=none]
[section1]

something = 132         # an integer
another = 5.131         # a real value
yetanother = true       # a boolean
finally = "some text"   # a string
\end{lstlisting}
\note{Options are NOT case-sensitive: \code{TwistShift} and \code{twistshift} are the same variable}

Have a look through the examples to see how the options are used.

\subsection{General options}

At the top of the BOUT.inp file (before any section headers), 
options which affect the core code are listed. 
These are common to all physics models, and the most useful of them are:
\begin{lstlisting}[language=bash,numbers=none]
NOUT = 100       # number of time-points output
TIMESTEP = 1.0   # time between outputs
\end{lstlisting}
which set the number of outputs, and the time step between them. Note that
this has nothing to do with the internal timestep used to advance the
equations, which is adjusted automatically. What time-step to use depends
on many factors, but for high-$\beta$ reduced MHD
ELM simulations reasonable choices are \code{1.0}
for the first part of a run (to handle initial transients), then around \code{10.0} for the
linear phase. Once non-linear effects become important, you will have to reduce the timestep to
around \code{0.1}.

Most large clusters or supercomputers have a limit on how long a job can
run for called ``wall time'', because it's the time taken according to a
clock on the wall, as opposed to the CPU time actually used.
If this is the case, you can use the option
\begin{lstlisting}[language=bash,numbers=none]
wall_limit = 10 # wall clock limit (in hours)
\end{lstlisting}
BOUT++ will then try to quit cleanly before this time runs out. Setting a
negative value (default is -1) means no limit.

Often it's useful to be able to restart a simulation from a chosen point,
either to reproduce a previous run, or to modify the settings and re-run.
A restart file is output every timestep, but this is overwritten each time,
and so the simulation can only be continued from the end of the last
simulation. Whilst it is possible to create a restart file from the output
data afterwards, it's much easier if you have the restart files. Using the
option
\begin{lstlisting}[language=bash,numbers=none]
archive = 20
\end{lstlisting}
saves a copy of the restart files every 20 timesteps, which can then be used as a starting point.

The X and Y size of the computational grid is set by the grid file, but the
number of points in the Z (axisymmetric) direction is specified in the options
file:
\begin{lstlisting}[language=bash,numbers=none]
MZ = 33
\end{lstlisting}
This must be $\texttt{MZ} = 2^n + 1$, and can be $2,3,5,9,\ldots$. The power of 2 is so
that FFTs can be used in this direction; the $+1$ is for historical reasons (inherited from BOUT)
and is going to be removed at some point.

Since the Z dimension is periodic, the domain size is specified as multiples or fractions of $2\pi$.
To specify a fraction of $2\pi$, use
\begin{lstlisting}[language=bash,numbers=none]
ZPERIOD = 10
\end{lstlisting}
This specifies a Z range from $0$ to $2\pi / \code{ZPERIOD}$, and is useful for simulation
of tokamaks to make sure that the domain is an integer fraction of a torus. If instead you want
to specify the Z range directly (for example if Z is not an angle), there are the options
\begin{lstlisting}[language=bash,numbers=none]
ZMIN = 0.0
ZMAX = 0.1
\end{lstlisting}
which specify the range in multiples of $2\pi$.

\note{For users of BOUT, the definition of \code{ZMIN} and \code{ZMAX} has been changed.
These are now fractions of $2\pi$ radians i.e. $\code{dz} = 2\pi(\code{ZMAX - ZMIN})/\code{(MZ-1)}$}

In BOUT++, grids can be split between processors in both X and Y directions. By
default only Y decomposition is used, and to use X decomposition you must specify
the number of processors in the X direction:
\begin{lstlisting}[language=bash,numbers=none]
NXPE = 1  # Set number of X processors
\end{lstlisting}

The grid file to use is specified relative to the root directory where the simulation
is run (i.e. running ``\code{ls ./data/BOUT.inp}'' gives the options file)
\begin{lstlisting}[language=bash,numbers=none]
grid = "data/cbm18_8_y064_x260.pdb"
\end{lstlisting}

\subsection{Time integration solver}
\label{sec:timeoptions}
\index{time integration}
BOUT++ can be compiled with several different time-integration solvers (see section~\ref{sec:solverlibrary}),
and at minimum should have Runge-Kutta (RK4) and PVODE (BDF/Adams) solvers available.

The solver library used is set using the \code{solver\_type} option, so either in BOUT.inp:
\begin{lstlisting}[language=bash,numbers=none]
[solver]
type = rk4  # Set the solver to use
\end{lstlisting}
or on the command line by adding \code{solver\_type=pvode} for example:
\begin{lstlisting}[language=bash,numbers=none]
mpirun -np 4 ./2fluid solver_type=rk4
\end{lstlisting}
{\bf NB}: Make sure there are no spaces around the ``='' sign: \code{solver\_type =pvode} won't work.
Table~\ref{tab:solvers} gives a list of time integration solvers, along with any compile-time
options needed to make the solver available.
\begin{table}[htb!]
\centering
\caption{Available time integration solvers}
\label{tab:solvers}
\begin{tabular}{c | c | c}
\hline
Name & Description & Compile options \\
\hline
rk4 & Runge-Kutta 4th-order explicit method & Always available \\
karniadakis & Karniadakis explicit method & Always available \\
pvode & 1998 PVODE with BDF method & Always available \\
cvode & SUNDIALS CVODE. BDF and Adams methods & --with-cvode \\
ida & SUNDIALS IDA. DAE solver & --with-ida \\
petsc & PETSc TS methods & --with-petsc \\
\hline
\end{tabular}
\end{table}

Each solver can have its own settings which work in slightly different ways, but some common
settings and which solvers they are used in are given in table~\ref{tab:solveropts}.
\begin{table}[htb!]
\centering
\caption{Time integration solver options}
\label{tab:solveropts}
\begin{tabular}{c | c | c}
\hline
Option & Description & Solvers used \\
\hline
atol & Absolute tolerance & rk4, pvode, cvode, ida \\
rtol & Relative tolerance & rk4, pvode, cvode, ida \\
mxstep & Maximum internal steps  & rk4 \\
       & per output step & \\
max\_timestep & Maxmimum timestep & rk4, cvode \\
start\_timestep & Starting guess for timestep & rk4 \\
timestep & Fixed timestep & karniadakis \\
use\_precon & Use a preconditioner? (Y/N) & pvode, cvode, ida \\
mudq, mldq & BBD preconditioner settings & pvode, cvode, ida \\
mukeep, mlkeep & & \\
maxl & & \\
use\_jacobian & Use user-supplied Jacobian? (Y/N) & cvode \\
adams\_moulton & Use Adams-Moulton method & cvode \\
 & rather than BDF & \\
\hline
\end{tabular}
\end{table}
The most commonly changed options are the  absolute and relative solver tolerances,
\code{ATOL} and \code{RTOL} which should be varied to check convergence.

\subsection{Laplacian inversion}
\index{Laplacian inversion}
A common problem in plasma models is to solve an equation of the form
\[
d\nabla^2_\perp x + \frac{1}{c}\nabla_\perp c\cdot\nabla_\perp x + a x = b
\]
where $x$ and $b$ are 3D variables, whilst $a$, $c$ and $d$ are 2D variables.
BOUT++ includes several routines for solving this equation; see the
developer's manual for details.

\begin{table}[htbp!]
\caption{Global Laplacian options}
\label{tab:lapopts}
\centering
\begin{tabular}[c]{c c c}
\hline
Option & Description & Default \\
\hline
\texttt{low\_mem} & Reduces memory usage & \texttt{false} \\
\texttt{use\_pdd} & Use the PDD algorithm & \texttt{false} \\
\texttt{all\_terms} & Include all terms & \texttt{false} \\
\texttt{laplace\_nonuniform} & Non-uniform mesh corrections & \texttt{false} \\
\texttt{filter} & Fraction of modes to filter & 0.2 \\
\texttt{max\_mode} & Maximum Z mode & filter \\ 
\hline
\end{tabular}
\end{table}

\subsection{Communications}
\index{communication}
The communication system has a section \code{[comms]}, with a true/false option \code{async}. This
determines whether asyncronous MPI sends are used; which method is faster varies (though not by much)
with machine and problem.

\subsection{Differencing methods}
\index{differencing}
Differencing methods are specified in three section (\code{[ddx]}, \code{[ddy]} and \code{[ddz]}), one
for each dimension. 
\begin{itemize}
\item \code{first}, the method used for first derivatives
\item \code{second}, method for second derivatives
\item \code{upwind}, method for upwinding terms
\item \code{flux}, for conservation law terms
\end{itemize}
The methods which can be specified are U1, U4, C2, C4, W2, W3, FFT
Apart from FFT, the first letter gives the type of method (U = upwind, C = central, W = WENO), and
the number gives the order.

\subsection{Model-specific options}

The options which affect a specific physics model vary, since they are defined in the physics module itself
(see section~\ref{sec:inputopts}). They should have a separate section, for example the high-$\beta$
reduced MHD code uses options in a section called \code{[highbeta]}.

There are three places to look for these options: the BOUT.inp file; the physics model C++ code, and 
the output logs. The physics module author should ideally have an example input file, with commented options
explaining what they do; alternately they may have put comments in the C++ code for the module. 
Another way is to look at the output logs: when BOUT++ is run, (nearly) all options used are printed 
out with their default values. This won't provide much explanation of what they do, but may be useful anyway.
See section~\ref{sec:output} for more details.

\subsection{Variable initialisation}
\index{variable initialisation}
Each variable being evolved has its own section, with the same name as the
output data. For example, the high-$\beta$ model has variables ``P'',
``jpar'', and ``U'', and so has sections \code{[P]}, \code{[jpar]},
\code{[U]} (not case sensitive). 

The shape of the initial value is specified for each dimension separately
using the options \code{xs\_opt}, \code{ys\_opt}, and \code{zs\_opt}. These
are set to an integer: 
\begin{enumerate}
\setcounter{enumi}{-1}
\item Constant (this is the default)
\item Gaussian, with a peak location given by \code{xs\_s0}, \code{ys\_s0},
  \code{zs\_s0} as a fraction of the domain (i.e. $0 \rightarrow 1$.
  The width is given by \code{*s\_wd}, also as a fraction of the domain size.
\item Sinusoidal, with the number of periods given by \code{*s\_mode}. 
\item Mix of mode numbers, with psuedo-random phases.
\end{enumerate}
The magnitude of the initial value is given by the variable \code{scale}.

Defaults for all variables can be set in a section called \code{[All]},
so for example the options below:
\begin{lstlisting}[language=bash,numbers=none]
[All]
scale = 0.0 # By default set variables to zero

xs_opt = 1  # Gaussian in X
ys_opt = 1  # Gaussian in Y
zs_opt = 2  # Sinusoidal in Z (axisymmetric direction)

xs_s0 = 0.5 # Peak in the middle of the X direction 
xs_wd = 0.1 # Width is 10% of the domain 

ys_s0 = 0.5 # Peak in the middle of the Y direction
ys_wd = 0.3 # Width is 30% of the Y domain

zs_mode = 3 # 3 periods in the Z direction

[U]
scale = 1.0e-5  # Amplitude for the U variable, overrides default
\end{lstlisting}

For field-aligned tokamak simulations, the Y direction is along the
field and in the core this will have a discontinuity at the twist-shift
location where field-lines are matched onto each other. To handle this,
a truncated Ballooning transformation can be used to construct a smooth
initial perturbation:
\[
U_0^{balloon} = \sum_{i=-N}^N F\left(x\right)G\left(y + 2\pi i\right)H\left(z + q2\pi i\right)
\]

\begin{figure}[h]
\includegraphics[width=0.48\textwidth, keepaspectratio]{figs/init_noballoon.png}
\includegraphics[width=0.48\textwidth, keepaspectratio]{figs/init_balloon.png}
\caption{Initial profiles in twist-shifted grid. {\bf Left}: Without ballooning transform, showing discontinuity at the matching location {\bf Right}: with ballooning transform}
\end{figure}

\note{The initial profiles code currently doesn't work very well for grids with branch-cuts (e.g. divertor tokamak), and will often have jumps which then make timesteps smaller}

\subsection{Boundary conditions}
\label{sec:bndryopts}
\index{boundary conditions}
Like the variable initialisation, boundary conditions can be set for
each variable in individual sections, with default values in a section
\code{[All]}. Boundary conditions are specified for each variable, being 
applied to variable itself during initialisation, and the time-derivatives
at each timestep.

When finding the boundary condition for a variable \code{var} on a boundary region, 
the options are checked in order from most to least specific:
\begin{itemize}
\item Section \code{var}, \code{bndry\_} + region name. Depending on the mesh file, regions
  of the grid are given labels. Currently
  these are \code{core}, \code{sol}, \code{pf} and \code{target} which
  are intended for tokamak edge simulations. Hence the variables checked
  are \code{bndry\_core}, \code{bndry\_pf} etc.
\item Section \code{var}, \code{bndry\_} + boundary side. These names are
  \code{xin}, \code{xout}, \code{yup} and \code{ydown}. 
\item Section \code{var}, variable \code{bndry\_all}
\item The same settings again except in section \code{All}.
\end{itemize}
The default setting for everything is therefore \code{bndry\_all}
in the \code{All} section.

Boundary conditions are given names, with optional arguments in brackets. 
Currently implemented boundary conditions are:
\begin{itemize}
\item \code{dirichlet} - Set to zero
\item \code{dirichlet(<number>)} - Set to some number e.g. \code{dirichlet(1)}
sets the boundary to $1.0$
\item \code{neumann} - Zero gradient
\item \code{zerolaplace} - Laplacian = 0, decaying solution (X boundaries only)
\item \code{constlaplace} - Laplacian = const, decaying solution (X boundaries only)
\end{itemize}
The zero- or constant-Laplacian boundary conditions works as follows:
\begin{eqnarray*}
\nabla_\perp^2 f &=& 0 \\
&\simeq& g^{xx}\frac{\partial^2 f}{\partial x^2} + g^{zz}\frac{\partial^2 f}{\partial z^2}
\end{eqnarray*}
which when Fourier transformed in $z$ becomes: 
\[
g^{xx}\frac{\partial^2 \hat{f}}{\partial x^2} - g^{zz}k_z^2 \hat{f} = 0
\]
which has the solution
\[
\hat{f} = Ae^{xk_z\sqrt{g^{zz}/g^{xx}}} + Be^{-xk_z\sqrt{g^{zz}/g^{xx}}}
\]
Assuming that the solution should decay away from the domain, on the inner
$x$ boundary $B = 0$, and on the outer boundary $A = 0$. 

\subsubsection{Relaxing boundaries}

All boundaries can be modified to be ``relaxing'' which are a combination
of zero-gradient time-derivative, and whatever boundary condition they are
applied to. The idea is that this prevents sharp discontinuities at boundaries
during transients, whilst maintaining the desired boundary condition on
longer timescales. In some cases this can improve the numerical stability
and timestep.

For example, \code{relax(dirichlet)} will make a field $f$ at point $i$ in the
boundary follow a point $i-1$ in the domain:
\[
\left.\deriv{f}{t}\right|_i = \left.\deriv{f}{t}\right|_{i-1}  - f_i / \tau
\]
where $\tau$ is a timescale for the boundary (currently set to 0.1, but will
be a global option).
When the time-derivatives are slow close to the boundary, the boundary
relaxes to the desired condition (Dirichlet in this case), but when the
time-derivatives are large then the boundary approaches Neumann to reduce
discontinuities.

By default, the relaxation rate is set to $10$ (i.e. a timescale of $\tau=0.1$).
To change this, give the rate as the second argument e.g. \code{relax(dirichlet, 2)} would relax to a Dirichlet boundary condition at a rate of $2$.

\subsubsection{Examples}

This example is taken from the UEDGE benchmark test (in \texttt{examples/uedge-benchmark}):
\begin{lstlisting}[language=bash,numbers=none]
[All]
bndry_all = neumann # Default for all variables, boundaries
  
[Ni]
bndry_target = neumann
bndry_core = relax(dirichlet(1.))   # 1e13 cm^-3 on core boundary
bndry_all  = relax(dirichlet(0.1))  # 1e12 cm^-3 on other boundaries

[Vi]
bndry_ydown = relax(dirichlet(-1.41648))   # -3.095e4/Vi_x
bndry_yup   = relax(dirichlet( 1.41648))
\end{lstlisting}

The variable \code{Ni} (density) is set to a Neumann boundary condition
on the targets (yup and ydown), relaxes towards $1$ on the core boundary,
and relaxes to $0.1$ on all other boundaries. Note that the \code{bndry\_target = neumann} needs to be in the \code{Ni} section: If we just had
\begin{lstlisting}[language=bash,numbers=none]
[All]
bndry_all = neumann # Default for all variables, boundaries

[Ni]
bndry_core = relax(dirichlet(1.))   # 1e13 cm^-3 on core boundary
bndry_all  = relax(dirichlet(0.1))  # 1e12 cm^-3 on other boundaries
\end{lstlisting}
then the ``target'' boundary condition for \code{Ni} would first search
in the \code{[Ni]} section for \code{bndry\_target}, then for \code{bndry\_all}
in the \code{[Ni]} section. This is set to \code{relax(dirichlet(0.1))}, not
the Neumann condition desired.

\section{Running BOUT++}
\label{sec:running}

The command to run a parallel code varies between machines, but on most small clusters
the following command should work:
\begin{verbatim}
mpirun -np <num. proc> <command>
\end{verbatim}
so to run the \code{highbeta\_reduced} case on 4 processors:
\begin{verbatim}
cd <path>/BOUT++/examples/highbeta_elm/
mpirun -np 4 ./highbeta_reduced
\end{verbatim}

\subsection{Startup output}

When BOUT++ is run, it produces a lot of output initially, mainly listing
the options which have been used so you can check that it's doing what you
think it should be. It's generally a good idea to scan over this
see if there are any important warnings or errors. Each processor outputs
its own log file \texttt{BOUT.log.\#}
and the log from processor 0 is also sent to the screen. This output may
look a little different if it's out of date, but the general layout will
probably be the same.

First comes the introductory blurb:
\begin{verbatim}
BOUT++ version 0.85
Revision: c8794400adc256480f72c651dcf186fb6ea1da49
MD5 checksum: 8419adb752f9c23b90eb50ea2261963c
Code compiled on May 11 2011 at 18:22:37

B.Dudson (University of York), M.Umansky (LLNL) 2007
Based on BOUT by Xueqiao Xu, 1999
\end{verbatim}
The version number (0.85 here) gets increased occasionally after
some major feature has been added. To help match simulations to code versions,
the Git revision of the core BOUT++ code and the date and time it was
compiled is recorded. Because code could be modified from the revision,
an MD5 checksum of all the code is also calculated. This information
makes it possible to verify precisely which version of the code was used
for any given run.

Next comes the compile-time options, which depend on how
BOUT++ was configured (see section~\ref{sec:installbout})
\begin{verbatim}
Compile-time options:
	Checking enabled, level 2
	Signal handling enabled
	PDB support disabled
	netCDF support enabled
\end{verbatim}
Thisn says that some run-time checking of values is enabled,
that the code will try to catch segmentation faults to print a useful
error, that PDB files aren't supported, but that NetCDF files are.

The processor number comes next:
\begin{verbatim}
Processor number: 0 of 1
\end{verbatim}
This will always be processor number '0' on screen as only the output
from processor '0' is sent to the terminal. After this comes some
the core BOUT++ code reads some options:
\begin{verbatim}
	Option /nout = 50 (data/BOUT.inp)
	Option /timestep = 100 (data/BOUT.inp)
	Option /grid = slab.6b5.r1.cdl (data/BOUT.inp)
	Option /dump_float = true   (default)
	Option /non_uniform = false (data/BOUT.inp)
	Option /restart = false  (default)
	Option /append = false  (default)
	Option /dump_format = nc (data/BOUT.inp)
	Option /StaggerGrids = false  (default)
\end{verbatim}
This lists each option and the value it has been assigned.
For every option the source of the value being used is also given.
If a value had been given on the command line then \texttt{(command line)}
would appear after the option.

\begin{verbatim}
Setting X differencing methods
	First       :  Second order central (C2)
	Second      :  Second order central (C2)
	Upwind      :  Third order WENO (W3)
	Flux        :  Split into upwind and central (SPLIT)
Setting Y differencing methods
	First       :  Fourth order central (C4)
	Second      :  Fourth order central (C4)
	Upwind      :  Third order WENO (W3)
	Flux        :  Split into upwind and central (SPLIT)
Setting Z differencing methods
	First       :  FFT (FFT)
	Second      :  FFT (FFT)
	Upwind      :  Third order WENO (W3)
	Flux        :  Split into upwind and central (SPLIT)
\end{verbatim}

This is a list of the differential methods for each direction. These are set in the 
BOUT.inp file (\code{[ddx]}, \code{[ddy]} and \code{[ddz]} sections), but can be overridden
for individual operators. For each direction, numerical methods can be specified for
first and second central difference terms, upwinding terms of the form
$\deriv{\partial f}{\partial t} = \underline{v}\cdot\nabla f$,
and flux terms of the form $\deriv{\partial f}{\partial t} = \nabla\cdot\left(\underline{v}f\right)$.
By default the flux terms are just split into a central and an upwinding term.

In brackets are the code used to specify the method in BOUT.inp. A list
of available methods is given in section~\ref{sec:diffmethod} on page~\pageref{sec:diffmethod}.

\begin{verbatim}
Setting grid format
	Option /grid_format =  (default)
	Using NetCDF format for file 'slab.6b5.r1.cdl'
Loading mesh
	Grid size: 10 by 64
	Option /mxg = 2 (data/BOUT.inp)
	Option /myg = 2 (data/BOUT.inp)
	Option /NXPE = 1 (default)
	Option /mz = 65 (data/BOUT.inp)
	Option /twistshift = false (data/BOUT.inp)
	Option /TwistOrder = 0 (default)
	Option /ShiftOrder = 0 (default)
	Option /shiftxderivs = false (data/BOUT.inp)
	Option /IncIntShear = false  (default)
	Option /BoundaryOnCell = false  (default)
	Option /StaggerGrids = false  (default)
	Option /periodicX = false  (default)
	Option /async_send = false  (default)
	Option /zmin = 0 (data/BOUT.inp)
	Option /zmax = 0.0028505 (data/BOUT.inp)
\end{verbatim}

\begin{verbatim}
WARNING: Number of inner y points 'ny_inner' not found. Setting to 32
\end{verbatim}
Optional quantities (such as \code{ny\_inner} in this case) which
are not specified are given a default (best-guess) value, and a warning is printed.
\begin{verbatim}
	EQUILIBRIUM IS SINGLE NULL (SND) 
	MYPE_IN_CORE = 0
	DXS = 0, DIN = -1. DOUT = -1
	UXS = 0, UIN = -1. UOUT = -1
	XIN = -1, XOUT = -1
	Twist-shift: 
\end{verbatim}
At this point, BOUT++ reads the grid file, and works out the topology of the grid,
and connections between processors.
BOUT++ then tries to read the metric coefficients from the grid file:
\begin{verbatim}
	WARNING: Could not read 'g11' from grid. Setting to 1.000000e+00
	WARNING: Could not read 'g22' from grid. Setting to 1.000000e+00
	WARNING: Could not read 'g33' from grid. Setting to 1.000000e+00
	WARNING: Could not read 'g12' from grid. Setting to 0.000000e+00
	WARNING: Could not read 'g13' from grid. Setting to 0.000000e+00
	WARNING: Could not read 'g23' from grid. Setting to 0.000000e+00
\end{verbatim}

These warnings are printed because the coefficients have not been specified in the 
grid file, and so the metric tensor is set to the default identity matrix.

\begin{verbatim}
	WARNING: Could not read 'zShift' from grid. Setting to 0.000000e+00
	WARNING: Z shift for radial derivatives not found
\end{verbatim}
To get radial derivatives, the quasi-ballooning coordinate method is used (see
section~\ref{sec:shiftcoords}). The upshot of this is that to get radial derivatives,
interpolation in Z is needed. This should also always be set to FFT.

\begin{verbatim}
	WARNING: Twist-shift angle 'ShiftAngle' not found. Setting from zShift
	Option /twistshift_pf = false  (default)
\end{verbatim}

\begin{verbatim}
	Maximum error in diagonal inversion is 0.000000e+00
	Maximum error in off-diagonal inversion is 0.000000e+00
\end{verbatim}
If only the contravariant components (\code{g11} etc.) of the metric tensor are specified,
the covariant components (\code{g\_11} etc.) are calculated by inverting the metric tensor matrix. 
Error estimates are then calculated by calculating $g_{ij}g^{jk}$ as a check.
Since no metrics were specified in the input, the metric tensor was set to the identity
matrix, making inversion easy and the error tiny.
\begin{verbatim}
	WARNING: Could not read 'J' from grid. Setting to 0.000000e+00
	WARNING: Jacobian 'J' not found. Calculating from metric tensor
\end{verbatim}

\begin{verbatim}
	Maximum difference in Bxy is 1.444077e-02
Calculating differential geometry terms
	Communicating connection terms
Boundary regions in this processor: core, sol, target, target, 
	done
\end{verbatim}

\begin{verbatim}
Setting file formats
	Using NetCDF format for file 'data/BOUT.dmp.0.nc'
\end{verbatim}

The laplacian inversion code is initialised, and prints out the options used.
\begin{verbatim}
Initialising Laplacian inversion routines
	Option comms/async = true   (default)
	Option laplace/filter = 0.2 (default)
	Option laplace/low_mem = false  (default)
	Option laplace/use_pdd = false  (default)
	Option laplace/all_terms = false  (default)
	Option laplace/laplace_nonuniform = false  (default)
	Using serial algorithm
	Option laplace/max_mode = 26 (default)
\end{verbatim}

After this comes the physics module-specific output:
\begin{verbatim}
Initialising physics module
	Option solver/type =  (default)
        .
        .
        .
\end{verbatim}
This typically lists the options used, and useful/important normalisation factors etc.

Finally, once the physics module has been initialised, 
and the current values loaded, the solver can be started
\begin{verbatim}
Initialising solver
	Option /archive = -1 (default)
	Option /dump_format = nc (data/BOUT.inp)
	Option /restart_format = nc (default)
	Using NetCDF format for file 'nc'
\end{verbatim}

\begin{verbatim}
Initialising PVODE solver
	Boundary region inner X
	Boundary region outer X
	3d fields = 2, 2d fields = 0 neq=84992, local_N=84992
\end{verbatim}
This last line gives the number of equations being evolved (in this case 84992),
and the number of these on this processor (here 84992).
\begin{verbatim}
	Option solver/mudq = 16 (default)
	Option solver/mldq = 16 (default)
	Option solver/mukeep = 0 (default)
	Option solver/mlkeep = 0 (default)
\end{verbatim}

The absolute and relative tolerances come next:
\begin{verbatim}
	Option solver/atol = 1e-10 (data/BOUT.inp)
	Option solver/rtol = 1e-05 (data/BOUT.inp)
\end{verbatim}

\begin{verbatim}
	Option solver/use_precon = false  (default)
	Option solver/precon_dimens = 50 (default)
	Option solver/precon_tol = 0.0001 (default)
	Option solver/mxstep = 500 (default)
\end{verbatim}


\begin{verbatim}
	Option fft/fft_measure = false  (default)
\end{verbatim}
This next option specifies the maximum number of internal timesteps which CVODE
will take between outputs.
\begin{verbatim}
	Option fft/fft_measure = false  (default)
Running simulation

Run started at  : Wed May 11 18:23:20 2011

	Option /wall_limit = -1 (default)
\end{verbatim}

\subsection{Per-timestep output}

At the beginning of a run, just after the last line in the previous section,
a header is printed out as a guide
\begin{verbatim}
Sim Time  |  RHS evals  | Wall Time |  Calc    Inv   Comm    I/O   SOLVER
\end{verbatim}

Each timestep (the one specified in BOUT.inp, not the internal timestep), 
BOUT++ prints out something like
\begin{verbatim}
1.001e+02         76       2.27e+02    87.1    5.3    1.0    0.0    6.6
\end{verbatim}

This gives the simulation time; the number of times the time-derivatives (RHS) were evaluated;
the wall-time this took to run, and percentages for the time spent in different parts of the code. 
\begin{itemize}
\item \code{Calc} is the time spent doing calculations such as multiplications, derivatives etc
\item \code{Inv} is the time spent in inversion code (i.e. inverting Laplacians), including any communication
which may be needed to do the inversion.
\item \code{Comm} is the time spent communicating variables (outside the inversion routine)
\item \code{I/O} is the time spent writing dump and restart files to disk. Most of the time this should not be an issue
\item \code{SOLVER} is the time spent in the implicit solver code.
\end{itemize}

The output sent to the terminal (not the log files) also includes a run time, and estimated
remaining time.

\section{Output and post-processing}
\label{sec:output}

The majority of the existing analysis and post-processing code is written
in IDL. The directory \texttt{idllib} contains many useful routines for
reading PDB files and analysing data. A summary of available IDL routines
is given in Appendix~\ref{apx:idl_routines}.

Post-processing using Python is also possible, and there are some modules
in the \texttt{pylib} directory, and a list of routines in Appendix~\ref{apx:py_routines}. This is a more recent addition, and so is
not yet as developed as the IDL support. 


\subsection{Note on reading PDB files}

IDL comes with routines to manipulate NetCDF files, but to read PDB files you will need the PDB2IDL library
supplied with BOUT++:
\begin{verbatim}
cd PDB2IDL
make
\end{verbatim}
To use the \code{PDB2IDL} library and IDL analysis codes, set the following environment variables
\begin{verbatim}
IDL_PATH=$IDL_PATH:<bout>/idllib/
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<bout>/lib/
\end{verbatim}
Before any of the PDB2IDL functions can be used, you first need to run
\begin{verbatim}
IDL> .r pdb2idl
\end{verbatim}
This can be added to your IDL startup file (which is specified by the
\code{IDL\_STARTUP} environment variable).

\subsection{Reading BOUT++ output into IDL}
\index{IDL}
There are several routines provided for reading data from BOUT++ output into
IDL. In the directory containing the BOUT++ output files
(usually \texttt{data/}), you can list the variables available using
\begin{verbatim}
IDL> print, file_list("BOUT.dmp.0.nc")
Ajpar Apar BOUT_VERSION MXG MXSUB MYG MYSUB MZ NXPE NYPE Ni Ni0 Ni_x Te0 Te_x
Ti0 Ti_x ZMAX ZMIN iteration jpar phi rho rho_s t_array wci
\end{verbatim}

The \code{file\_list} procedure just returns an array, listing all the variables in a given file. This
method (and all the file\_ methods) works for both NetCDF and PDB files.

One thing new users can find confusing is that different simulations may have very different outputs.
This is because {\bf BOUT++ is not a single physics model}: the variables evolved and written to file
are determined by the model, and will be very different between (for example) full MHD and reduced Braginskii models.
There are however some variables which all BOUT++ output files contain:
\begin{itemize}
\item \code{BOUT\_VERSION}, which gives the version number of BOUT++ which produced the file. This is mainly to help output processing codes handle changes to the output file format. For example, BOUT++ version 0.30 introduced 2D domain decomposition which needs to be handled when collecting data.
\item \code{MXG},\code{MYG}. These are the sizes of the X and Y guard cells
\item \code{MXSUB}, the number of X grid points in each processor. This does not include the guard cells, so the total X size of each field will be \code{MXSUB + 2*MXG}.
\item \code{MYSUB}, the number of Y grid points per processor (like MXSUB)
\item \code{MZ}, the number of Z points
\item \code{NXPE, NYPE}, the number of processors in the X and Y directions. \code{NXPE * MXSUB + 2*MXG= NX}, \code{NYPE * MYSUB = NY}
\item \code{ZMIN}, \code{ZMAX}, the range of Z in fractions of $2\pi$.
\item \code{iteration}, the last timestep in the file
\item \code{t\_array}, an array of times
\end{itemize}
Most of these - particularly those concerned with grid size and processor layout - 
are used by post-processing routines such as \code{collect}, and are seldom needed
directly.
To read a single variable from a file, there is the \code{file\_read} function:
\begin{verbatim}
IDL> wci = file_read("BOUT.dmp.0.nc", "wci")
IDL> print, wci
  9.58000e+06
\end{verbatim}
\note{The \code{file\_read} command (and NetCDF/PDB access generally) is case-sensitive: variable \code{Wci} is different to \code{wci}}

To read in all the variables in a file into a structure, use the \code{file\_import} function:
\begin{verbatim}
IDL> d = file_import("BOUT.dmp.0.nc")
IDL> print, d.wci
  9.58000e+06
\end{verbatim}
This is often used to read in the entire grid file at once. Doing this for
output data files can take a long time and use a lot of memory.

Reading from individual files is fine for scalar quantities and time arrays, but reading arrays
which are spread across processors (i.e. evolving variables) is tedious
to do manually. Instead, there is the \code{collect} function to automate this:
\begin{verbatim}
IDL> ni = collect(var="ni")
Variable 'ni' not found
-> Variables are case-sensitive: Using 'Ni'
Reading from .//BOUT.dmp.0.nc: [0-35][2-6] -> [0-35][0-4]
\end{verbatim}
This function takes care of the case, so that reading ``ni'' is automatically corrected to ``Ni''. The result is a 4D variable:
\begin{verbatim}
IDL> help, ni
NI              FLOAT     = Array[36, 5, 64, 400]
\end{verbatim}
with the indices \code{[X, Y, Z, T]}. Note that in the output files,
these variables are stored in \code{[T, X, Y, Z]} format instead but
this is changed by \code{collect}.
Sometimes you don't want to read in the entire array (which may be very large). To read in only a subset, there are several optional keywords with \code{[min,max]}
ranges:
\begin{verbatim}
IDL> ni = collect(var="Ni", xind=[10,20], yind=[2,2], zind=[0,31], tind=[300,399])
Reading from .//BOUT.dmp.0.nc: [10-20][4-4] -> [10-20][2-2]
IDL> help, ni
NI              FLOAT     = Array[11, 1, 32, 100]
\end{verbatim}

\subsection{Summary of IDL file routines}

Functions file\_* can read/write either PDB or NetCDF
files, depending on the file extension. Hence same
analysis / pre-processing codes can use PDB and/or
NetCDF files. Any file ending ".nc", ".cdl", ".cdf"
is assumed to be NetCDF, otherwise PDB.

Open a PDB or NetCDF file:
\begin{verbatim}
handle = file_open("filename", /write, /create) 
\end{verbatim}

Array of variable names:
\begin{verbatim}
list = file_list(handle)
list = file_list("filename")
\end{verbatim}

Number of dimensions:
\begin{verbatim}
nd = file_ndims(handle, "variable")
nd = file_ndims("filename", "variable")
\end{verbatim}

Read a variable from file. Inds = [xmin, xmax, ymin, ymax, ...]
\begin{verbatim}
data = file_read(handle, "variable", inds=inds)
data = file_read("filename", "variable", inds=inds)
\end{verbatim}

Write a variable to file. For NetCDF it tries to match up
dimensions, and defines new dimensions when needed
\begin{verbatim}
status = file_write(handle, "variable", data)
\end{verbatim}

Close a file after use
\begin{verbatim}
file_close, handle
\end{verbatim}

To read in all the data in a file into a structure:
\begin{verbatim}
data = file_import("filename")
\end{verbatim}

and to write a structure to file:
\begin{verbatim}
status = file_export("filename, data)
\end{verbatim}

Converting file types can now be done using
\begin{verbatim}
d = file_import("somefile.pdb")
s = file_export("somefile.nc", d)
\end{verbatim}

Note that this will mess up the case of the variable names,
and names may be changed to become valid IDL variable names.
To convert PDB files to NetCDF there is also pdb2cdf in bout/archiving/

\subsection{IDL analysis routines}

Now that the BOUT++ results have been read into IDL, all the usual analysis 
and plotting routines can be used. In addition, there are many useful routines
included in the \texttt{idllib} subdirectory. There is a \texttt{README}
file which describes what each of these routines, but some of the most useful
ones are listed here. All these examples assume there is a variable \code{P}
which has been read into IDL as a 4D [x,y,z,t] variable:

\begin{itemize}
\item \code{fft\_deriv} and \code{fft\_integrate} which differentiate and integrate
periodic functions.
\item \code{get\_integer}, \code{get\_float}, and \code{get\_yesno} request integers, floats and a yes/no answer from the user respectively.
\item \code{showdata} animates 1 or 2-dimensional variables. Useful for quickly displaying results in different ways. This is useful for taking a quick look at the data,
but can also produce bitmap outputs for turning into a movie for presentation. 
To show an animated surface plot at a particular poloidal location (32 here):
\begin{verbatim}
IDL> showdata, p[*,32,*,*]
\end{verbatim}
To turn this into a contour plot,
\begin{verbatim}
IDL> showdata, p[*,32,*,*], /cont
\end{verbatim}
To show a slice through this at a particular toroidal location (0 here):
\begin{verbatim}
IDL> showdata, p[*,32,0,*]
\end{verbatim}
There are a few other options, and ways to show data using this code; see the README
file, or comments in \texttt{showdata.pro}. Instead of plotting to screen, showdata can
produce a series of numbered bitmap images by using the \code{bmp} option
\begin{verbatim}
IDL> showdata, p[*,32,*,*], /cont, bmp="result_"
\end{verbatim}
which will produce images called \texttt{result\_0000.bmp}, \texttt{result\_0001.bmp}
and so on. Note that the plotting should not be obscured or minimised, since this
works by plotting to screen, then grabbing an image of the resulting plot.

\item \code{moment\_xyzt} takes a 4D variable (such as those from \code{collect}),
and calculates RMS, DC and AC components in the Z direction.
\item \code{safe\_colors} A general routine for IDL which arranges the color table so that colors are numbered 1 (black), 2 (red), 3 (green), 4 (blue). Useful for plotting, and
used by many other routines in this library.
\end{itemize}

There are many other useful routines in the \texttt{idllib} directory. See the \texttt{idllib/README} file
for a short description of each one.

\subsection{Python routines}
\index{Python}
The NumPy module enables Python to efficiently handle large arrays of 
scientific data. There are several modules available for reading
NetCDF files, but the one used by the BOUT++ routines is ...
Because this library may change in future, or BOUT++ may use a different
file format, file access is wrapped into a class DataFile. This
provides a simple interface for reading and writing files.

\section{Generating input grids}
\label{sec:gridgen}

The grid file describes the grid points, geometry, and starting profiles.
Currently BOUT++ supports either NetCDF or PDB format binary files.
During startup, BOUT++ looks in the grid file for the following variables.
If any are not found, a warning will be printed and the default values used.
\begin{itemize}
\item X and Y grid sizes (integers) \code{nx} and \code{ny} {\bf REQUIRED}
\item Differencing quantities in 2D arrays \code{dx[nx][ny]} and \code{dy[nx][ny]}.
  If these are not found they will be set to 1.
\item Diagonal terms of the metric tensor $g^{ij}$ \code{g11[nx][ny]}, \code{g22[nx][ny]},
  and \code{g33[nx][ny]}. If not found, these will be set to 1. \index{metric tensor}
\item Off-diagonal metric tensor $g^{ij}$ elements \code{g12[nx][ny]}, \code{g13[nx][ny]},
  and \code{g23[nx][ny]}. If not found, these will be set to 0.
\item Z shift for sheared grids \code{zshift[nx][ny]}. This is intended for dpsi derivatives
  in sheared coordinates. If not found, set to zero.
\end{itemize}
The remaining quantities determine the topology of the grid. These are based on tokamak
single/double-null configurations, but can be adapted to many other situations.
\begin{itemize}
\item Separatrix locations \code{ixseps1}, and \code{ixseps2} If neither is given, both
  are set to nx (i.e. all points in closed ``core'' region). If only \code{ixseps1} is found,
  \code{ixseps2} is set to nx, and if only ixseps2 is found, ixseps1 is set to -1.
\item Branch-cut locations \code{jyseps1\_1}, \code{jyseps1\_2}, \code{jyseps2\_1}, and
  \code{jyseps2\_2}
\item Twist-shift matching condition \code{twistshift[nx]}. This is applied in the ``core''
  region between indices \code{jyseps2\_2}, and \code{jyseps1\_1 + 1}, if enabled in the options
  file. If not given, this is set to zero.
\end{itemize} 

\note{All input quantities should be normalised - no normalisation is performed by
the BOUT++ code. Normalisation can be performed in the initialisation code, provided a call to
\code{geometry()} is made after any changes to the metrics. \\
For users of BOUT, the radial derivative is \code{dx = dpsi / (bmag/1e4)}}

The only quantities which are required are the sizes of the grid. If these are the only
quantities specified, then the coordinates revert to cartesian.


This section describes how to generate inputs for tokamak equilibria. If you're not interested
in tokamaks then you can skip to the next section.

The directory \texttt{tokamak\_grids} contains code to generate input grid files for tokamaks.
These can be used by the \code{2fluid} and \code{highbeta\_reduced} modules, and are (mostly)
compatible with inputs to the BOUT-06 code. 

Figure~\ref{fig:gridgen} shows the routines and file formats used in taking
output from different codes and converting into input to BOUT++.
\begin{figure}[htbp!]
\centering
\includegraphics[width=0.7\paperwidth, keepaspectratio]{figs/grid_gen.pdf}
\caption{Generation of BOUT++ grid files. In red are the file formats, and in black the conversion routines. Blue are external codes.}
\label{fig:gridgen}
\end{figure}

\subsection{Converting grid files}
\index{ELITE}\index{GATO}
Currently conversions exist for ELITE \code{.eqin} and GATO \code{dskgato} equilibrium files.
Conversion of these into BOUT++ input grids is in two stages: In the first, both these
input files are converted into a common PDB format which describes the Grad-Shafranov equilibrium.
These intermediate files are then converted to BOUT++ grids using an interactive IDL script.


\subsection{Generating equilibria}

The directory \texttt{tokamak\_grids/shifted\_circle} contains IDL code to generate shifted circle
(large aspect ratio) Grad-Shafranov equilibria.

\subsection{Running pdb2bout}

There are many options which are set interactively, so here's a run-through of the code (only showing most important outputs):

\begin{verbatim}
IDL> pdb2bout, "cbm18_dens6.dskgato.pdb", output="test.pdb"
***Maximum mu0p is       23071.7
Is this pressure (not mu0*pressure)?
\end{verbatim}
This is needed because although many grid formats claim to store $\mu_0 P$, they actually store $P$. Since the given maximum value is very large, it must be in Pascals, so answer yes (y).

The grid will then be displayed along with the safety factor and pressure
profiles against normalised $\psi$. In all three plots, a red line marks the
location of the plasma edge. You must then choose the radial domain in normalised $\psi$:
\begin{verbatim}
Inner Psi boundary:0.6
Outer Psi boundary:1.2
Number of radial grid points:           69
Of which inside the plasma:           49
Is this range ok?
\end{verbatim}
The plots will now also show two green lines for the inner and outer boundaries. Enter no (n) to specify a different range.

The code then checks to see if any plasma density of temperatures have been set in the input file:
\begin{verbatim}
====== SETTING PLASMA PROFILES =======
Some plasma parameters given in input file
Use given parameters?
\end{verbatim}
Saying yes will just use the given values, but saying no will give you some more options:
\begin{verbatim}
Generating plasma profiles:
  1. Flat temperature profile
  2. Flat density profile
  3. Te proportional to density
Profile option:
\end{verbatim}
The procedure is the same for each of these, so taking option 2 (flat density):
\begin{verbatim}
Setting flat density profile
Density [10^20 m^-3]:1.0
\end{verbatim}
Anything can be entered here, depending on what you want to simulate. The code will ensure that whatever you enter, the equilibrium pressure is maintained. In this case the temperature is calculated from pressure and the specified density.
\begin{verbatim}
Maximum temperature (eV):      720.090
Is this ok?
\end{verbatim}
NOTE: This is the maximum temperature anywhere on the input grid (i.e. in this case at $\psi = 0$), not just inside the chosen domain. Entering no will go back and you can specify a different density.

Earlier the radial resolution was printed, in this case 69 grid points. You can now change this if you need to:
\begin{verbatim}
Increase radial resolution?
\end{verbatim}
Although it says increase, you can also decrease the resolution. Entering yes will allow you to enter a different number of radial grid points. It's recommended to use $2^n + 4$ grid points because this makes it easier to decompose the grid (4 cells for boundary, remainder equally divided between processors).

At this point, an orthogonal grid is generated:
\begin{verbatim}
======== GENERATING ORTHOGONAL COORDINATES FOR BOUT ========= 
Number of poloidal grid points: 64
Enter x index of equal hthe [0, 68] :35
\end{verbatim}
Using  $2^n$ points is highly recommended, but only because the points must be equally divided between processors.
The x index of equal hthe shouldn't matter very much except for highly shaped plasmas. Recommend that you set it somewhere around the peak pressure gradient, or middle of the grid.
\begin{verbatim}
Interpolating Rxy
Interpolating Zxy
Is this ok?
\end{verbatim}
Two plots are shown: On the left the original mesh, and on the right the new orthogonal mesh. If this doesn't look right you can enter `no' and change the poloidal resolution and location of equal $h_\theta$.

\begin{verbatim}
Add vacuum region?n
\end{verbatim}
This is a little experimental, and extends the grid into vacuum. This is useful if the equilibrium supplied doesn't include a vacuum region. In this case we already have a vacuum region, so can answer no.

You're now presented with several options:
\begin{verbatim}
Equilibrium correction options:
  0  No correction
  1  RBt using force balance
  2  hthe and RBt using force balance and q (FAILS)
  3  hthe and RBt using force balance and jpar
Enter option:
\end{verbatim}
Because the input Grad-shafranov solution is probably not perfect to begin with, and has now been interpolated onto a new grid, the force balance in ballooning coordinates is not quite satisfied. These options attempt to correct the equilibrium slightly to ensure force balance, the given $q$ profile, and the given $j_{||}$ profile. Option 1 can sometimes work well, other times it can fail to converge (as in this case). It's safe to just use option 0.

\begin{verbatim}
Calculating poloidal arc length dthe
Maximum difference in hthe:       0.23551123
Maximum percentage difference:        16.745859
Use new hthe?
\end{verbatim}
A key metric in the BOUT/BOUT++ coordinates is the poloidal arc length $h_\theta$. A plot will shot this quantity calculated geometrically (solid line), and calculated by enforcing force balance (red symbols) at the outboard midplane.
The difference between these two methods is an indication of the quality of the Grad-shafranov solution. Entering 'y' will use the ``new'' $h_\theta$ calculated from force balance, whilst 'n' will use the $h_\theta$ calculated geometrically. Personally. i prefer to make sure force balance is satisfied so enter 'y'.

\begin{verbatim}
Checking parallel current
****Equilibrium has -ve toroidal field
\end{verbatim}
Because of the varied and confusing ways different codes define the poloidal
and toroidal directions, this code currently just sets Bp and Bt positive,
and then uses the expression for Jpar to work out what sign Bt should have.
This is fine if you just want an equilibrium, but for detailed comparison to
experiment where the sign of Bt may/will make a difference this needs to be
changed.

Jpar calculated from quantities such as Bp, Bt and hthe is now shown as red symbols, with the jpar from the original Grad-shafranov solution as a black line. Like the hthe display, this is a good consistency check.
\begin{verbatim}
Use new Jpar?
\end{verbatim}
Entering 'y' will use the calculated jpar i.e. consistent with the other grid quantities, but probably more noisy and slightly different to the original. Entering 'n' will use the original jpar profiles.

\begin{verbatim}
q is negative. Reversing values from equilibrium
\end{verbatim}
This can be printed because the $q$ profile given in the grid file is almost always positive, whereas qsafe calculated by integrating the pitch angle can be positive or negative. In this case the toroidal field has been set negative (see above), and so qinty is negative too.

\begin{verbatim}
Use new qsafe?
\end{verbatim}
As with hthe and jpar, the qsafe specified in the original grid file is plotted as a black line, and the value calculated by integrating quantities on the new mesh is shown as red symbols. Entering 'y' uses the values consistent on the new grid, whilst 'n' uses the original safety factor profile. In most cases i'd prefer the grid to be consistent, rather than being identical to the input, so answer 'y'. You may have to do some experimentation though.

\begin{verbatim}
****Minimum pressure is very small:       0.0000000
****Setting minimum pressure to 1% of maximum
\end{verbatim}
This is because having negative pressures is very bad for BOUT/BOUT++ runs, and can easily be caused by overshoots or even rounding error when the pressure is too low. Because the equilibrium doesn't depend on absolute pressure, this just adds a constant pressure across the entire profile.

Finally, the grid file is written to PDB format
\begin{verbatim}
Cannot write 2 dimensional double dx. Writing as float
.
.
.
\end{verbatim}
These warnings are because the PDB2IDL library currently doesn't have any functions for writing doubles, and pdb2bout does calculations in double precision. The output is therefore converted to single-precision floats.

\section{Fluid equations}
\label{sec:equations}

Once you have tried some example codes, and generally got the hang of running
BOUT++ and analysing the results, there will probably come a time when
you want to change the equations being solved. 
This section uses the ideal MHD equations as an example, demonstrating how
a BOUT++ physics module is put together. It assumes you have a working knowledge
of C or C++, but you don't need to be an expert - most of the messy code is hidden away
from the physics module. There are several good books on C and C++, but I'd recommend online
tutorials over books because there are a lot more of them, they're quicker to scan through,
and they're cheaper.

When going through this section, it may help to refer to the finished code, which
is given in the file \texttt{mhd.cxx} in the BOUT++ examples directory. The equations to be solved are:
\begin{eqnarray*}
\deriv{\rho}{t} &=& -\mathbf{v}\cdot\nabla\rho - \rho\nabla\cdot\mathbf{v} \\
\deriv{p}{t} &=& -\mathbf{v}\cdot\nabla p - \gamma p\nabla\cdot\mathbf{v} \\
\deriv{\mathbf{v}}{t} &=& -\mathbf{v}\cdot\nabla\mathbf{v} + \frac{1}{\rho}\left(-\nabla p + \left(\nabla\times\mathbf{B}\right)\times\mathbf{B}\right) \\
\deriv{\mathbf{B}}{t} &=& \nabla\times\left(\mathbf{v}\times\mathbf{B}\right)
\end{eqnarray*}

To specify a set of equations to solve in BOUT++, two functions need to be written:
\begin{lstlisting}
int physics_init(bool restarting)
{
  return 0;
}

int physics_run(BoutReal t)
{
  return 0;
}
\end{lstlisting}
The first of these is called once at the start of the simulation, and should set up the problem,
specifying which variables are to be evolved. The argument \code{restarting} is false the first time a problem is run, and true if loading the state from a
restart file.

The second function \code{physics\_run} is called every time-step,
and should calculate the time-derivatives for a given state. In both cases returning non-zero
tells BOUT++ that an error occurred.

\subsection{Variables}

We need to define the variables to evolve as global variables (so they can be used in
\code{physics\_init} and \code{physics\_run}.

\note{Version 0.85 and earlier needed two variables to be defined, so if you're
upgrading then you can remove the time-derivative variables}

For ideal MHD, we need two 3D scalar fields density $\rho$ and pressure $p$, and two
3D vector fields velocity $v$, and magnetic field $B$:
\begin{lstlisting}
Field3D rho, p; // 3D scalar fields
Vector3D v, B;  // 3D vector fields

int physics_init(bool restarting)
{
}
\end{lstlisting}

Scalar and vector fields behave much as you would expect: \lstinline!Field3D! objects can be added, subtracted,
multiplied, divided and exponentiated, so the following examples are all valid operations: 
\index{Field3D}\index{BoutReal}
\begin{lstlisting}
Field3D a, b, c;
BoutReal r;

a = b + c; a = b - c;
a = b * c; a = r * b;
a = b / c; a = b / r; a = r / b;
a = b ^ c; a = b ^ r; a = r ^ b;
\end{lstlisting}
Similarly, vector objects can be added/subtracted from each other, multiplied/divided by scalar
fields and real numbers, for example:\index{Vector3D}
\begin{lstlisting}
Vector3D a, b, c;
Field3D f;
BoutReal r;

a = b + c; a = b - c;
a = b * f; a = b * r;
a = b / f; a = b / r;
\end{lstlisting}
In addition the dot and cross products are represented by \code{*} and \pow symbols:
\begin{lstlisting}
Vector3D a, b, c;
Field3D f;

f = a * b // Dot-product
a = b ^ c // Cross-product
\end{lstlisting}

For both scalar and vector field operations, so long as the result of an operation is of the correct type,
the usual C/C++ shorthand notation can be used:
\begin{lstlisting}
Field3D a, b;
Vector3D v, w;

a += b; v *= a; v -= w; v ^= w; // valid
v *= w; // NOT valid: result of dot-product is a scalar
\end{lstlisting}

%\parbox{\textwidth}

\note{In C++ the \pow operator has lower precedence than the \code{*} or \code{+} operators.
To be safe, always put exponentiation and cross-product operations in brackets}



\subsection{Evolution equations}

At this point we can tell BOUT++ which variables to evolve, and where the state and time-derivatives
will be stored. This is done using the \code{bout\_solve(variable, name)} function
in \code{physics\_init}:
\begin{lstlisting}
int physics_init(bool restarting)
{
  bout_solve(rho, "density");
  bout_solve(p,   "pressure");
  bout_solve(v,   "v");
  bout_solve(B,   "B");
  
  return 0;
}
\end{lstlisting}
The name given to this function will be used in the output and restart data files. These will be
automatically read and written depending on input options (see section~\ref{subsec:options}).
Input options based on these names are also used to initialise the variables.

If the name of the variable in the output file is the same as the variable name, you can use 
a shorthand macro. In this case, we could use this shorthand for \code{v} and \code{B}:\index{SOLVE\_FOR}
\begin{lstlisting}
SOLVE_FOR(v);
SOLVE_FOR(B);
\end{lstlisting}
To make this even shorter, we can use macros \code{SOLVE\_FOR2}, \code{SOLVE\_FOR3}, ..., \code{SOLVE\_FOR6}
to shorten our initialisation code to
\begin{lstlisting}
int physics_init(bool restarting)
{
  bout_solve(rho, "density");
  bout_solve(p,   "pressure");
  SOLVE_FOR2(v, B);
  
  return 0;
}
\end{lstlisting}

The equations to be solved can now be written in the \code{physics\_run} function. The value
passed to the function (\code{BoutReal t}) is the simulation time - only needed if your
equations contain time-dependent sources or similar terms. To refer to the time-derivative
of a variable \code{var}, use \code{ddt(var)}. The ideal MHD equations can be written as:
\begin{lstlisting}
int physics_run(BoutReal t)
{
  ddt(rho) = -V_dot_Grad(v, rho) - rho*Div(v);
  ddt(p) = -V_dot_Grad(v, p) - gamma*p*Div(v);
  ddt(v) = -V_dot_Grad(v, v) + ( (Curl(B)^B) - Grad(p) ) / rho;
  ddt(B) = Curl(v^B);
}
\end{lstlisting}
Where the differential operators \code{vector = Grad(scalar)}, \code{scalar = Div(vector)}, and
\code{vector = Curl(vector)} are used. For the density and pressure equations, 
the $\mathbf{v}\cdot\nabla\rho$ term could be written as \code{v*Grad(rho)}, but this would then use
central differencing in the Grad operator. Instead, the function \code{V\_dot\_Grad} uses upwinding
methods for these advection terms. In addition, the \code{Grad} function will not operate
on vector objects (since result is neither scalar nor vector), so the $\mathbf{v}\cdot\nabla\mathbf{v}$ term CANNOT be written as \code{v*Grad(v)}. 

\subsection{Input options}
\label{sec:inputopts}
\index{Options}
Note that in the above equations the extra parameter \code{gamma} has been used. To enable this to be set
in the input options file (see section~\ref{sec:options}), we use the \code{options} object in the
initialisation function:
\begin{lstlisting}
BoutReal gamma;

int physics_init(bool restarting)
{
  Options *globalOptions = Options::getRoot();
  Options *options = globalOptions->getSection("mhd");

  options->get("gamma", gamma, 5.0/3.0);
\end{lstlisting}
This specifies that an option called ``gamma'' in a section called ``mhd'' should be put into the variable \code{gamma}. If the option could not be found, or was of the wrong type, the variable should be set to a default value of $5/3$.
The value used will be printed to the output file, so if gamma is not set in the input file the following line will appear:
\begin{verbatim}
      Option mhd / gamma = 1.66667 (default)
\end{verbatim}
This function can be used to get integers and booleans. To get strings, there
is the function (\code{char* options.getString(section, name)}.
To separate options specific to the physics model, these options should be put in a separate
section, for example here the ``mhd'' section has been specified. To save having to write the section name for every option, there is the \code{setSection} function:
\begin{lstlisting}
BoutReal gamma;
int someint;

int physics_init(bool restarting)
{
  Options *globalOptions = Options::getRoot();
  Options *options = globalOptions->getSection("mhd");

  options->get("gamma", gamma, 5.0/3.0);
  options->get("someint", someint, 0);
\end{lstlisting}
Most of the time, the name of the variable (e.g. \code{gamma}) will be
the same as the identifier in the options file (``gamma''). In this case,
there is the macro
\begin{lstlisting}[numbers=none]
OPTION(options, gamma, 5.0/3.0);
\end{lstlisting}
which is equivalent to
\begin{lstlisting}[numbers=none]
options->get("gamma", gamma, 5.0/3.0);
\end{lstlisting}
See section~\ref{sec:options} for more details of how to use the input options.

\subsection{Communication}
\index{communication}
If you plan to run BOUT++ on more than one processor, any operations involving y derivatives
will require knowledge of data stored on other processors. To handle the necessary parallel
communication, there is the \code{mesh->communicate} function. This takes care of where the data
needs to go to/from, and only needs to be told which variables to transfer.

If you only need to communicate a small number (up to 5 currently) of variables then just
call the \code{mesh->communicate} function directly. For the MHD code, we need to communicate
the variables \code{rho,p,v,B} at the beginning of the \code{physics\_run} function
before any derivatives are calculated:
\begin{lstlisting}
int physics_run(BoutReal t)
{
  mesh->communicate(rho, p, v, B);
\end{lstlisting}

If you need to communicate lots of variables, or want to change at run-time which
variables are evolved (e.g. depending on input options), then you can create a group
of variables and communicate them later.
To do this, first create a \code{FieldGroup} object\index{FieldGroup}, in this case called \code{comms},
then use the add method. This method does no communication, but records which variables to transfer
when the communication is done later.
\begin{lstlisting}
FieldGroup comms;

int physics_init() {
  .
  .
  .
  comms.add(rho);
  comms.add(p);
  comms.add(v);
  comms.add(B);

  return 0;
}
\end{lstlisting}

The \lstinline!comms.add()! routine can be given up to 6 variables at once (there's no practical
limit on the total number of variables which are added to a \lstinline!FieldGroup!), so this can be
shortened to
\begin{lstlisting}
FieldGroup comms;

int physics_init() {
  .
  .
  .
  comms.add(rho, p, v, B);

  return 0;
}
\end{lstlisting}

To perform the actual communication, call the \code{mesh->communicate} function
with the group. In this case
we need to communicate all these variables before performing any calculations, so call
this function at the start of the \code{physics\_run} routine:
\begin{lstlisting}
int physics_run(BoutReal t)
{
  mesh->communicate(comms);
  .
  .
  .
\end{lstlisting}
In many situations there may be several groups of variables which can be communicated at
different times. The function \code{mesh->communicate} consists of a call to \code{mesh->send} followed by \code{mesh->wait}
which can be done separately to interleave calculations and communications. This will speed up the
code if parallel communication bandwidth is a problem for your simulation.

In our MHD example, the calculation of \code{ddt(rho)} and \code{ddt(p)} does not require
\code{B}, so we could first communicate \code{rho}, \code{p}, and \code{v}, send \code{B}
and do some calculations whilst communications are performed: \index{comm\_handle}
\begin{lstlisting}
int physics_run(BoutReal t) {
  mesh->communicate(rho, p, v); // sends and receives rho, p and v
  comm_handle ch = mesh->send(B);// only send B
  
  ddt(rho) = ...
  ddt(p) = ...
  
  mesh->wait(ch); // now wait for B to arrive
  
  ddt(v) = ...
  ddt(B) = ...

  return 0;
}
\end{lstlisting}

This scheme is not used in \texttt{mhd.cxx}, partly for clarity, and partly because currently
communications are not a significant bottleneck (too much inefficiency elsewhere!).

\note{Before using the result of a differential operator as input to another differential operator,
communications must be performed for the intermediate result}

When a differential is calculated, points on neighbouring cells are assumed to be in the guard cells.
There is no way to calculate the result of the differential in the guard cells, and so after every 
differential operator the values in the guard cells are invalid. Therefore, if you take
the output of one differential operator and use it as input to another differential operator, you must 
perform communications (and set boundary conditions) first. See section~\ref{sec:diffops}.

\subsection{Boundary conditions}
\index{boundary conditions}
All evolving variables have boundary conditions applied automatically after
the \lstinline!physics_run! has finished. Which condition is applied depends on the options file
settings (see section~\ref{sec:bndryopts}). Quite often however it's necessary to impose
boundary conditions on other quantities.

The simplest way to do this is to specify a boundary condition as text, so to apply a Dirichlet boundary
condition:
\begin{lstlisting}
  Field3D var;
  ...
  var.applyBoundary("dirichlet");
\end{lstlisting}
The format is exactly the same as in the options file. Each time this is called it must
parse the text, create and destroy boundary objects. To avoid this overhead and have different
boundary conditions for each region, it's better to set the boundary conditions you want to use
first in \code{physics\_init}, then just apply them every time:
\begin{lstlisting}
Field3D var;

int physics_init() {
  ...
  var.setBoundary("myVar");
  ...
}

int physics_run(BoutReal t) {
  ...
  var.applyBoundary(); 
  ...
}
\end{lstlisting}
This will look in the options file for a section called \code{"[myvar]"}  (upper or lower case
doesn't matter) in the same way that evolving variables are handled. In fact this is precisely what
is done: inside \code{bout\_solve} (or \code{SOLVE\_FOR}) the \code{setBoundary} method is called,
and then after \code{physics\_run} the applyBoundary() method is called on each evolving variable.

Finally, another way to set the boundaries is to copy them from another variable:
\begin{lstlisting}
Field3D a, b;
  ...
  a.setBoundaryTo(b); // Copy b's boundaries into a
  ...
\end{lstlisting}

\subsection{Initial profiles}
\index{variable initialisation}
Up to this point the code is evolving total density, pressure etc. This has advantages for clarity,
but has problems numerically: For small perturbations, rounding error and tolerances in the time-integration
mean that linear dispersion relations are not calculated correctly. The solution to this
is to write all equations in terms of an initial ``background'' quantity and a time-evolving
perturbation, for example $\rho\left(t\right) \rightarrow \rho_0 + \tilde{\rho}\left(t\right)$.
For this reason, {\bf the initialisation of all variables passed to the \code{bout\_solve} function
is a combination of small-amplitude gaussians and waves; the user is expected to have performed this
separation into background and perturbed quantities.} 

To read in a quantity from a grid file, there is the \code{grid.get} function:

\begin{lstlisting}
Field2D Ni0; // Background density

int physics_init(bool restarting)
{
  ...
  mesh->get(Ni0, "Ni0");
  ...
}
\end{lstlisting}

As with the input options, most of the time the name of the variable in the
physics code will be the same as the name in the grid file to avoid confusion.
In this case, you can just use \index{GRID\_LOAD}
\begin{lstlisting}
GRID_LOAD(Ni0);
\end{lstlisting}
which is equivalent to
\begin{lstlisting}
mesh->get(Ni0, "Ni0");
\end{lstlisting}

\subsection{Output variables}
BOUT++ always writes the evolving variables to file, but often it's useful to add other variables
to the output. For convenience you might want to write the normalised starting profiles or other
non-evolving values to file. For example:
\begin{lstlisting}
  Field2D Ni0;
  ...
  GRID_LOAD(Ni0);
  dump.add(Ni0, "Ni0", 0); 
\end{lstlisting}
where the '0' at the end means the variable should only be written to file once at the start of the
simulation. For convenience there are some macros e.g. \index{SAVE\_ONCE}
\begin{lstlisting}
  SAVE_ONCE(Ni0); 
\end{lstlisting}
is equivalent to
\begin{lstlisting}
  dump.add(Ni0, "Ni0", 0); 
\end{lstlisting}


\section{Fluid equations 2: reduced MHD}

The MHD example presented previously covered some of the functions available in BOUT++,
which can be used for a wide variety of models. There are however several other significant
functions and classes which are commonly used, which will be illustrated using another example

\begin{lstlisting}
Field3D 
int physics_init(bool restarting) {
  
}

int physics_run(BoutReal t) {
  
}
\end{lstlisting}

\subsection{Printing messages/warnings}
\label{sec:printing}

In order to print to screen and/or a log file, the object \code{output} is provided. 
This provides two different ways to write output: the C (\code{printf}) way, and the
C++ stream way. This is because each method can be clearer in different circumstances,
and people have different tastes in these matters. 

The C-like way (which is the dominant way in BOUT++) is to use the \code{write} function,
which works just like \code{printf}, and takes all the same codes (it uses \code{sprintf}
internally). \index{output}
\begin{lstlisting}
output.write(const char *format, ...)
\end{lstlisting}
For example:
\begin{lstlisting}
output.write("This is an integer: %d, and this a real: %e\n", 5, 2.0)
\end{lstlisting}

For those who prefer the C++ way of doing things, a completely equivalent way is to
treat \code{output} as you would \code{cout}:
\begin{lstlisting}
output << "This is an integer: " << 5 << ", and this a real: " << 2.0 << endl;
\end{lstlisting}
which will produce exactly the same result as the \code{output.write} call above.

On all processors, anything sent to \code{output} will be written to a log file called
\texttt{BOUT.log.\#} with \# replaced by the processor number. On processor 0, anything
written to the output will be written to screen (stdout), in addition to the log file.
Unless there is a really good reason not to, please use this \code{output} object 
when writing text output.

\subsection{Laplacian inversion}
\index{Laplacian inversion}
Quite a common problem in plasma simulation codes is to invert an equation of the form
\[
\nabla_\perp^2 x + a x = b
\]
where $a$ is symmetric in z, and the operator $\nabla_\perp = \nabla - \mathbf{b}\left(\mathbf{b}\cdot\nabla\right) = -\mathbf{b\times} \left(\mathbf{b\times}\nabla\right)$. For example, this operator
appears in reduced MHD for the vorticity inversion and $j_||$.
Efficiently inverting this operator is done in the same way as in the BOUT code:
FFTs are used in the $z$ direction to transform this problem into a set of 1D 
inversion problems (in $x$) for each Fourier mode.
These inversion problems are band-diagonal (tri-diagonal in the case of 2nd-order differencing) and so
inversions are very efficient: $O\left(n_z \log n_z\right)$ for the FFTs, $O\left(n_x\right)$ for tridiagonal inversion
using the Thomas algorithm \cite{press-1999}, where $n_x$ and $n_z$ are the number of grid-points
in the $x$ and $z$ directions respectively.
This routine is:
\begin{lstlisting}
invert_laplace(b, x, flags, &a);
\end{lstlisting}
where \code{b} and \code{x} are 3D fields, and \code{a} is a pointer to a 2D field (which may be
NULL). \code{flags} is an \code{int} which determines boundary conditions and other options.
Its value is calculated by adding the settings given in \code{invert\_laplace.hxx}, and reproduced in
table~\ref{tab:laplaceflags}.
\begin{table}[htbp!]
\caption{Laplacian inversion flags: add the required quantities together.}
\label{tab:laplaceflags}
\centering
\begin{tabular}[c]{c | l}
\hline
Flag & Meaning \\
\hline
1 & Zero-gradient DC on inner (X) boundary. Default is zero-value \\
2 & Zero-gradient AC on inner boundary \\
4 & Zero-gradient DC on outer boundary \\
8 & Zero-gradietn AC on outer boundary \\
16 & Zero DC component everywhere \\
32 & Not used currently \\
64 & Set width of boundary to 1 (default is \code{MXG}) \\
128 & Use 4$^{th}$-order band solver (default is 2$^{nd}$ order tridiagonal) \\
256 & Attempt to set zero laplacian AC component on inner boundary by combining \\
    & 2nd and 4th-order differencing at the boundary. \\
    & Ignored if tridiagonal solver used. \\
512 & Zero laplacian AC on outer boundary \\
1024 & Symmetric boundary condition on inner boundary \\
2048 & Symmetric outer boundary condition \\
\hline
\end{tabular}
\end{table}

\subsection{Error handling}

Finding where bugs have occurred in a (fairly large) parallel code is a difficult problem.
This is more of a concern for developers of BOUT++ (see the developers manual), but it is still
useful for the user to be able to hunt down bug in their own code, or help narrow down where
a bug could be occurring.

If you have a bug which is easily reproduceable i.e. it occurs almost immediately every time
you run the code, then the easiest way to hunt down the bug is to insert lots of \code{output.write}
statements (see section~\ref{sec:printing}). Things get harder when a bug only occurs after a long time
of running, and/or only occasionally. For this type of problem, a useful tool
can be the message stack.
At the start of a section of code, put a message onto the stack: \index{msg\_stack}
\begin{lstlisting}
   msg_stack.push("Some message here");
\end{lstlisting}
which can also take arguments in \code{printf} format, as with \code{output.write}. At the end of the section of code, take the message off the stack again:
\begin{lstlisting}
   msg_stack.pop();
\end{lstlisting}
If an error occurs, the message stack is printed out, and this can then
help track down where the error originated.


\section{Differential operators}
\label{sec:diffops}

There are a huge number of possible ways to perform differencing in computational
fluid dynamics, and BOUT++ is intended to be able to implement a large number of them. This
means that the way differentials are handled internally is quite involved; see the
developer's manual for full gory details. Much of the time this detail is not all that
important, and certainly not while learning to use BOUT++. Default options are
therefore set which work most of the time, so you can start using the code without
getting bogged down in these details.

In order to handle many different differencing methods and operations, many layers are
used, each of which handles just part of the problem. The main division is between
differencing methods (such as 4th-order central differencing), and differential operators
(such as $\nabla_{||}$).

\subsection{Differencing methods}
\label{sec:diffmethod}
\index{differencing}
Methods are implemented on 5-point stencils, and are divided into three categories:
\begin{itemize}
\item Central-differencing methods, for diffusion operators $\frac{df}{dx}$, $\frac{d^2f}{dx^2}$. Each method has a short code, and currently include
  \begin{itemize}
  \item \texttt{C2}: 2$^{nd}$ order $f_{-1} - 2f_0 + f_1$
  \item \texttt{C4}: 4$^{th}$ order $\left(-f_{-2} + 16f_{-1} - 30f_0 + 16f_1 - f_2\right)/12$ 
  \item \texttt{W2}: 2$^{nd}$ order CWENO
  \item \texttt{W3}: 3$^{rd}$ order CWENO
  \item \texttt{FFT}: Fourier Transform method in Z (axisymmetric) direction only
  \end{itemize}
\item Upwinding methods for advection operators $v_x\frac{df}{dx}$
  \begin{itemize}
  \item \texttt{U1}: 1$^{st}$ order upwinding
  \item \texttt{U4}: 4$^{th}$ order upwinding
  \item \texttt{W3}: 3$^{rd}$ order Weighted Essentially Non-Oscillatory (WENO)\cite{jiang-1997}
  \end{itemize}
\item Flux conserving and limiting methods for terms of the form $\frac{d}{dx}\left(v_x f\right)$
  \begin{itemize}
  \item \texttt{SPLIT}: split into upwind and central terms $\frac{d}{dx}\left(v_x f\right) = v_x\frac{df}{dx} + f\frac{dv_x}{dx}$
  \item \texttt{NND}: Non-oscillatory, containing No free parameters and Dissipative (NND) scheme\cite{nnd-2010}
  \end{itemize}
\end{itemize}

Both of these methods avoid overshoots (Gibbs phenomena) at sharp gradients such as shocks,
but the simple 1st-order method has very large artificial diffusion. WENO schemes are
a development of the ENO reconstruction schemes which combine good handling of sharp-gradient
regions with high accuracy in smooth regions.

\subsection{Non-uniform meshes}
\index{non-uniform mesh}
Setting \texttt{non\_uniform = true} in the BOUT.inp options file
enables corrections to second derivatives in $X$ and $Y$. This correction
is given by writing derivatives as:
\[
\deriv{f}{x} \simeq \frac{1}{\Delta x} \deriv{f}{i}
\]
where $i$ is the cell index number. The second derivative is therefore given by
\[
\frac{\partial^2 f}{\partial x^2} \simeq \frac{1}{\Delta x^2}\frac{\partial^2 f}{\partial i^2} + \frac{1}{\Delta x}\deriv{f}{x} \cdot \deriv{}{i}\left(\frac{1}{\Delta x}\right)
\]
The correction factor $\partial/\partial i\left(1/\Delta x\right)$ can be
calculated automatically, but you can also specify \texttt{d2x} in the grid file which is
\[
\texttt{d2x} = \deriv{\Delta x}{i} = \frac{\partial^2 x}{\partial i^2}
\]
The correction factor is then calculated from \texttt{d2x} using
\[
\deriv{}{i}\left(\frac{1}{\Delta x}\right) = -\frac{1}{\Delta x^2} \deriv{\Delta x}{i}
\]


\subsection{Operators}

These are differential operators which are independent of the coordinate system used.
\[
\begin{array}{rclrcl}
\mathbf{v} &=& \nabla f &\qquad \code{Vector} &=& \code{Grad(Field)} \\
f &=& \nabla\cdot\mathbf{a} &\qquad \code{Field} &=& \code{Div(Vector)} \\
\mathbf{v} &=& \nabla\times\mathbf{a} &\qquad \code{Vector} &=& \code{Curl(Vector)} \\
f &=& \mathbf{v}\cdot\nabla g &\qquad \code{Field} &=& \code{V\_dot\_Grad(Vector, Field)} \\
\mathbf{v} &=& \mathbf{a}\cdot\nabla\mathbf{b} &\qquad \code{Vector} &=& \code{V\_dot\_Grad(Vector, Vector)} \\
f &=& \nabla^2 f &\qquad \code{Field} &=& \code{Laplacian(Field)}
\end{array}
\]

\begin{eqnarray*}
\nabla\phi &=& \deriv{\phi}{u^i}\nabla u^i \Rightarrow \left(\nabla\phi\right)_i = \deriv{\phi}{u^i} \\
\nabla\cdot A &=& = \frac{1}{J}\deriv{}{u^i}\left(Jg^{ij}A_j\right) \\
\nabla^2\phi &=& \Gamma^i\deriv{\phi}{u^i} + g^{ij}\frac{\partial^2\phi}{\partial u^i\partial u^j} \\
& &\Gamma^i = \frac{1}{J}\deriv{}{u^i}\left(Jg^{ij}\right)
\end{eqnarray*}

Another set of operators assume that the equilibrium magnetic field is written
in Clebsh form as \index{Clebsh}
\[
\mathbf{B}_0 = \nabla z\times\nabla x \qquad \left|B_0\right| = \frac{\sqrt{g_{yy}}}{J}
\]

\begin{eqnarray*}
\partial^0_{||} &=& \mathbf{b}_0\cdot\nabla = \frac{1}{\sqrt{g_{yy}}}\deriv{}{y} \\
\nabla^0_{||}F &=& B_0\partial^0_{||}\left(\frac{F}{B_0}\right) \\
\partial^2_{||}\phi &=& \partial^0_{||}\left(\partial^0_{||}\phi\right) = \frac{1}{\sqrt{g_{yy}}}\deriv{}{y}\left(\frac{1}{\sqrt{g_{yy}}}\right)\deriv{\phi}{y} + \frac{1}{g_{yy}}\frac{\partial^2\phi}{\partial y^2} \\
\mathbf{b}_0\cdot\nabla\phi\times\nabla A &=& \frac{1}{J\sqrt{g_{yy}}}\left[\left(g_{yy}\deriv{\phi}{z} - g_{yz}\deriv{\phi}{y}\right)\deriv{A}{x} + \left(g_{yz}\deriv{\phi}{x} - g_{xy}\deriv{\phi}{z}\right)\deriv{A}{y} + \left(g_{xy}\deriv{\phi}{y} - g_{yy}\deriv{\phi}{x}\right)\deriv{A}{z}\right]
\end{eqnarray*}

\[
\nabla_\perp \equiv \nabla - \underline{b}\left(\underline{b}\cdot\nabla\right) \qquad \underline{b}\cdot\nabla = \frac{1}{JB}\frac{\partial}{\partial y}
\]
\[
\underline{b} = \frac{1}{JB}\underline{e}_y = \frac{1}{JB}\left[g_{xy}\nabla x + g_{yy}\nabla y + g_{yz}\nabla z\right]
\]
In a Clebsch coordinate system $\underline{B} = \nabla z \times \nabla x = \frac{1}{J}\underline{e}_y$, $g_{yy} = \underline{e}_y\cdot\underline{e}_y = J^2B^2$, and so the $\nabla y$ term cancels out:
\begin{eqnarray*}
\nabla_\perp &=& \nabla x\left(\deriv{}{x} - \frac{g_{xy}}{\left(JB\right)^2}\deriv{}{y}\right) \\
&+& \nabla z\left(\deriv{}{z} - \frac{g_{yz}}{\left(JB\right)^2}\deriv{}{y}\right)
\end{eqnarray*}

\subsection{Setting differencing method}

\section{Staggered grids}
\label{sec:staggergrids}
\index{staggered grids}
Until now all quantities have been cell-centred i.e. both velocities
and conserved quantities were defined at the same locations. This
is because these methods are simple and this was the scheme used in 
the original BOUT. This class of methods can however be susceptable to
grid-grid oscillations, and so most shock-capturing schemes involve
densities and velocities (for example) which are not defined at the same location: their
grids are staggered.

By default BOUT++ runs with all quantities at cell centre. To enable staggered grids,
set
\begin{verbatim}
StaggeredGrids = true
\end{verbatim}
in the top section of the \texttt{BOUT.inp} file.

There are four possible locations in a grid cell where a quantitity can be defined in BOUT++:
centre, lower X, lower Y, and lower Z. These are illustrated in figure~\ref{fig:stagLocations}.
\begin{figure}[htbp!]
\centering
\includegraphics[width=0.4\paperwidth, keepaspectratio]{figs/stagLocations.pdf}
\caption{Locations in a grid cell where quantities may be defined.}
\label{fig:stagLocations}
\end{figure}

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.4\paperwidth, keepaspectratio]{figs/stagArith.pdf}
\caption{How the cell location of an arithmetic operation (\code{+,-,*,/,\pow}) is decided}
\label{fig:stagArith}
\end{figure}


\section{Examples}
\label{sec:examples}

The code and input files in the \texttt{examples/} subdirectory are for research, demonstrating BOUT++, 
and to check for broken functionality. Some proper unit tests have been implemented, but this
is something which needs improving. The examples which were published in \cite{Dudson2009,dudson-2008-arxiv}
were \texttt{drift-instability}, \texttt{interchange-instability} and \texttt{orszag-tang}.


\subsection{advect1d}

The model in \texttt{gas\_compress.cxx} solves the compressible gas dynamics equations for
the density $n$, velocity $\mathbf{V}$, and pressure $P$:

\subsection{drift-instability}

The physics code \texttt{2fluid.cxx} implements a set of reduced Braginskii 2-fluid equations,
similar to those solved by the original BOUT code.
This evolves 6 variables: Density, electron and ion temperatures, parallel ion velocity,
parallel current density and vorticity.

Input grid files are the same as the original BOUT code, but the output format is different.

\begin{figure}[htbp!]
\centering
\subfigure[Growth rate]{
  \label{fig:drift_imag}
  \includegraphics[scale=0.35]{figs/drift_growth.pdf}
}
\subfigure[Real Frequency]{
  \label{fig:drift_real}
  \includegraphics[scale=0.35]{figs/drift_freq.pdf}
}
\caption{Resistive Drift wave instability test. Dashed lines are analytic results, diamonds from BOUT++ simulations}
\label{fig:drift_test}
\end{figure}

\subsection{em-drift}

\subsection{gyro-gem}

\subsection{interchange-instability}

\begin{figure}[htb!]
\centering
\includegraphics[scale=0.4]{figs/interchange_inst_test.pdf}
\caption{Interchange instability test. Solid lines are from analytic theory, symbols from BOUT++ simulations, and the RMS
density is averaged over $z$. Vertical dashed line marks the reference point, where analytic and simulation results are set equal}
\label{fig:profiles}
\end{figure}

\subsection{jorek-compare}

\subsection{lapd-drift}

\subsection{orszag-tang}

The file \texttt{mhd.cxx} solves the full MHD equations for the full values (perturbation + initial),
whilst the file \texttt{mhd\_perturb.cxx} solves for a perturbation about the equilibrium.

\subsection{shear-alfven-wave}

\subsection{sod-shock}

\begin{figure}[h]
\includegraphics[width=0.48\textwidth, keepaspectratio]{figs/sod_result.pdf}
\caption{Sod shock-tube problem for testing shock-handling methods}
\end{figure}

\subsection{uedge-benchmark}

\section{Notes}

\subsection{Compile options}

Compiling with \code{-DCHECK} enables a lot of checks of operations performed by the field
objects. This is very useful for debugging a code, and can be omitted once bugs have been removed.

For (sometimes) more useful error messages, there is the \code{-DTRACK} option. This
keeps track of the names of variables and includes these in error messages.

\subsection{Adaptive grids}

Two types of adaptive grids can be used in BOUT++: Moving meshes, and changing resolution.

\subsubsection{Moving meshes}

During either the initialisation, or the simulation itself, the metric tensors can be modified. This could
be used to make the coordinate system time-dependent. Since currently the metric tensors are 2D
fields, this would only allow axi-symmetric motion. Changing the tensors to be 3D objects is however
possible with fairly small modification to the code.

Whenever one of the metrics $g^{ij}$ are changed, a call to \code{geometry()} must be made.

\subsubsection{Changing resolution}

\note{Not implemented yet - this just for discussion}

Since all 2D and 3D fields/vectors are located internally in global lists, the resolution
of the grid can be changed when required by interpolation. {\bf This requires a new, more
efficient implementation of the Fields classes}.

\bibliography{references}
\bibliographystyle{unsrt}

\appendix

\section{Useful quantities}

\subsection{Divergence of ExB velocity}

\[
\underline{v}_{ExB} = \frac{\underline{b}\times\nabla\phi}{B}
\]
Using
\[
\nabla\cdot\left(\underline{F}\times\underline{G}\right) = \left(\nabla\times\underline{F}\right)\cdot\underline{G} - \underline{F}\cdot\left(\nabla\times\underline{G}\right)
\]
the divergence of the $\underline{E}\times\underline{B}$ velocity
can be written as 
\begin{equation}
\nabla\cdot\left(\frac{1}{B}\underline{b}\times\nabla\phi\right) = \left[\nabla\times\left(\frac{1}{B}\underline{b}\right)\right]\cdot\nabla\phi - \frac{1}{B}\underline{b}\cdot\nabla\times\nabla\phi
\label{eq:exb1}
\end{equation}
The second term on the right is identically zero (curl of a gradient). The first term on the right can be expanded as
\[
\left[\nabla\times\left(\frac{1}{B}\underline{b}\right)\right]\cdot\nabla\phi = \left[\nabla\left(\frac{1}{B}\right)\times\underline{b} + \frac{1}{B}\nabla\times\underline{b}\right]\cdot\nabla\phi
\]
Using
\[
\underline{b}\times\underline{\kappa} = \nabla\times\underline{b} - \underline{b}\left[\underline{b}\cdot\left(\nabla\times\underline{b}\right)\right]
\]
this becomes:
\begin{eqnarray*}
  \nabla\cdot\left(\frac{1}{B}\underline{b}\times\nabla\phi\right) = &-&\underline{b}\times\nabla\left(\frac{1}{B}\right)\cdot\nabla\phi \\
  &+& \frac{1}{B}\underline{b}\times\underline{\kappa}\cdot\nabla\phi \\
  &+& \left[\underline{b}\cdot\left(\nabla\times\underline{b}\right)\right]\underline{b}\cdot\nabla\phi
\end{eqnarray*}

Alternatively, equation~\ref{eq:exb1} can be expanded as
\begin{eqnarray*}
  \nabla\cdot\left(\frac{1}{B}\underline{b}\times\nabla\phi\right) &=& -B\underline{b}\times\nabla\left(\frac{1}{B^2}\right)\cdot\nabla\phi + \frac{1}{B^2}\nabla\times\underline{B}\cdot\nabla\phi \\
  &=& -B\underline{b}\times\nabla\left(\frac{1}{B^2}\right)\cdot\nabla\phi + \frac{1}{B^2}\underline{J}\cdot\nabla\phi
\end{eqnarray*}

\section{Installing PACT}
\label{apx:pact}
\index{PACT}
There are two ways to install PACT, and usually one of them will 
work on a given system. 

\subsection{Self-extracting package}

This is probably the easiest method (when it works). Download one of the 
``Executable UNIX distribution files'' from the PACT website and run:
\begin{verbatim}
./pact07_07_18-src -sl -i $HOME/local/
\end{verbatim}
The ``-sl'' flag tells it to generate shared libraries. If you don't plan on using IDL
to read/write PDB files, then you can omit this. The ``-i \$HOME/local/'' tells PACT to install
in your home directory/local.

If this script fails, you will usually have to resort to either trying to understand
DSYS, or going with the second method below.

\subsection{PACT source distribution}

The second method is to use a .tar.gz PACT source file. Here the version used
is \texttt{pact-2.1.0.tar.gz}.

\begin{verbatim}
 ~/ $ cd install
 ~/install/ $ tar -xzvf pact-2.1.0.tar.gz
 ~/install/ $ cd pact-2.1.0/
 ~/install/pact-2.1.0/ $ ./configure --prefix=$HOME/local --enable-shared
\end{verbatim}

\note{On Franklin, PACT will compile without the --enable-shared option, but not with it. This is OK if you just want to run BOUT++, but the shared libraries are needed for reading the results into IDL (the PDB2IDL library)}

At this point, the installation may fail with the following error:
\begin{verbatim}
configure: WARNING: yacc is a symbolic link to bison
configure: WARNING: bison is not a supported type of yacc
configure: error: No working yacc found
\end{verbatim}

If this happens, you need to first install Berkeley Yacc into your home directory
\begin{verbatim}
 ~/install/ $ ls
byacc.tar.gz       netcdf-tar -xzvf byacc.tar.gz4.0.1.tar.gz  pact-2.1.0.tar.gz
fftw-3.2.1.tar.gz  pact-2.1.0           sundials-2.4.0.tar.gz

 ~/install/ $ tar -xzvf byacc.tar.gz
 ~/install/ $ cd byacc-20080826/
 ~/install/byacc-20080826/ $ ./configure --prefix=$HOME/local
 ~/install/byacc-20080826/ $ gmake
 ~/install/byacc-20080826/ $ mkdir ~/local/bin
 ~/install/byacc-20080826/ $ cp yacc ~/local/bin/
\end{verbatim}
NB: We're copying the yacc executable manually because ``gmake install'' doesn't seem to work,
and the fix which works for PACT (see later) doesn't work here.

Add this directory to your path:
\begin{verbatim}
 ~/install/byacc-20080826/ $ setenv PATH $HOME/local/bin:$PATH
\end{verbatim}
You can check that this has worked by running ``which yacc'', which should then print
your home directory /local/bin/yacc.
You could also add this to your .profile startup scripts. Now go back to PACT:
\begin{verbatim}
 ~/install/byacc-20080826/ $ cd ../pact-2.1.0
 ~/install/pact-2.1.0/ $ ./configure --prefix=$HOME/local --enable-shared
 ~/install/pact-2.1.0/ $ gmake
 ~/install/pact-2.1.0/ $ gmake install
\end{verbatim}

The last step may fail with a strange error message like:
\begin{verbatim}
    The current directory must be set to the ITT directory.
    Change the default to the ITT directory and re-run
    this script.
\end{verbatim}
This happens when the wrong ``install'' is being used. Check by running:
\begin{verbatim}
 ~/install/byacc-20080826/ $ which install
\end{verbatim}
This should print ``/usr/bin/install'', but if not then run
\begin{verbatim}
 ~/install/byacc-20080826/ $ ln -s /usr/bin/install ~/local/bin/
 ~/install/pact-2.1.0/ $ ./configure --prefix=$HOME/local
 ~/install/pact-2.1.0/ $ gmake
 ~/install/pact-2.1.0/ $ gmake install
\end{verbatim}
NOTE: configure needs to be run again after messing with install.

This should now install PACT into your local directory.

\section{Compiling and running under AIX}

Most development and running of BOUT++ is done under Linux, with the occasional FreeBSD and OSX.
The configuration scripts are therefore heavily tested on these architectures. IBM's POWER
architecture however runs AIX, which has some crucial differences which make compiling a pain.

\begin{itemize}
\item Under Linux/BSD, it's usual for a Fortran routine \code{foo} to appear
  under C as \code{foo\_}, whilst under AIX the name is unchanged
\item MPI compiler scripts are usually given the names \code{mpicc} and
  either \code{mpiCC} or \code{mpicxx}. AIX uses \code{mpcc} and \code{mpCC}.
\item Like BSD, the \code{make} command isn't compatible with GNU make,
  so you have to run \code{gmake} to compile everything.
\item The POWER architecture is big-endian, different to the little endian 
  Intel and AMD chips. This can cause problems with binary file formats.
\end{itemize}

\subsection{SUNDIALS}

To compile SUNDIALS, use 
\begin{verbatim}
$ export CC=cc
$ export CXX=xlC
$ export F77=xlf
$ export OBJECT_MODE=64
$ ./configure --prefix=$HOME/local/ --with-mpicc=mpcc --with-mpif77=mpxlf CFLAGS=-maix64
\end{verbatim}

You may get an error message like:
\begin{verbatim}
make: Not a recognized flag: w
\end{verbatim}
This is because the AIX \code{make} is being used, rather than \code{gmake}.
The easiest way to fix this is to make a link to \code{gmake} in your local
bin directory:
\begin{verbatim}
$ ln -s /usr/bin/gmake $HOME/local/bin/make
\end{verbatim}
Running \code{which make} should now point to this \code{local/bin/make},
and if not then you need to make sure that your bin directory appears first
in the \code{PATH}:
\begin{verbatim}
export PATH=$HOME/local/bin:$PATH
\end{verbatim}

If you see an error like this:
\begin{verbatim}
ar: 0707-126 ../../src/sundials/sundials_math.o is not valid with the current object file mode.
        Use the -X option to specify the desired object mode.
\end{verbatim}
then you need to set the environment variable \code{OBJECT\_MODE}
\begin{verbatim}
export OBJECT_MODE=64
\end{verbatim}

Configuring BOUT++, you may get the error:
\begin{verbatim}
configure: error: C compiler cannot create executables
\end{verbatim}
In that case, you can try using:
\begin{verbatim}
./configure CFLAGS="-maix64"
\end{verbatim}

When compiling, you may see warnings
\begin{verbatim}
xlC_r: 1501-216 (W) command option -64 is not recognized - passed to ld
\end{verbatim}

At this point, the main BOUT++ library should compile, and you can try
compiling one of the examples. 

\begin{verbatim}
ld: 0711-317 ERROR: Undefined symbol: .NcError::NcError(NcError::Behavior)
ld: 0711-317 ERROR: Undefined symbol: .NcFile::is_valid() const
ld: 0711-317 ERROR: Undefined symbol: .NcError::~NcError()
ld: 0711-317 ERROR: Undefined symbol: .NcFile::get_dim(const char*) const
\end{verbatim}

This is probably because the NetCDF libraries are 32-bit, whilst BOUT++ has been compiled as 64-bit. 
You can try compiling BOUT++ as 32-bit:
\begin{verbatim}
$ export OBJECT_MODE=32
$ ./configure CFLAGS="-maix32"
$ gmake
\end{verbatim}
If you still get undefined symbols, then go back to 64-bit, and edit make.config, raplacing
\code{-lnetcdf\_c++} with {-lnetcdf64\_c++}, and \code{-lnetcdf} with {-lnetcdf64}. This 
can be done by running:
\begin{verbatim}
$ sed 's/netcdf/netcdf64/g' make.config > make.config.new
$ mv make.config.new make.config
\end{verbatim}

\section{BOUT++ functions (alphabetical)}

This is a list of functions which can be called by users writing
a physics module. For a full list of functions, see the Reference manual,
DOxygen documentation, and source code.

\begin{itemize}
  \item \texttt{Field = {\bf abs}(Field | Vector)}
  \item \texttt{(Communicator).{\bf{add}}(Field | Vector)} \\
    Add a variable to a communicator object.
  \item \texttt{{\bf apply\_boundary}(Field. ``name'')}
  \item \texttt{Field = {\bf b0xGrad\_dot\_Grad}(Field, Field, CELL\_LOC)}
  \item \texttt{{\bf bout\_solve}(Field, Field, ``name'')}
  \item \texttt{{\bf bout\_solve}(Vector, Vector, ``name'')}
  \item \texttt{(Communicator).{\bf{clear}}()} \\
    Remove all variables from a Communicator object
  \item \texttt{Field = {\bf cos}(Field)}
  \item \texttt{Field = {\bf cosh}(Field)}
  \item \texttt{Vector = {\bf Curl}(Vector)}
  \item \texttt{Field = {\bf Delp2}(Field)} \\
    $\nabla_\perp^2$ operator
  \item \texttt{Field = {\bf Div}(Vector)} \\
    Divergence of a vector
  \item \texttt{Field = {\bf Div\_par}(Field f)} \\
    Parallel divergence $B_0\mathbf{b}\cdot\nabla\left(f / B_0\right)$
  \item \texttt{{\bf dump.add}(Field, ``name'', 1/0)}
  \item \texttt{Field = {\bf filter}(Field, modenr)}
  \item \texttt{{\bf geometry\_derivs}()} \\
    Calculates useful quantities from the metric tensor. Call this
    every time the metric tensor is changed.
  \item \texttt{Vector = {\bf Grad}(Field)}
  \item \texttt{Field = {\bf Grad\_par}(Field)}
  \item \texttt{Field = {\bf Grad2\_par2}(Field)}
  \item \texttt{{\bf grid\_load}(BoutReal, ``name'')} \\
    Load a scalar real from the grid file
  \item \texttt{{\bf grid\_load2d}(Field2D, ``name'')} \\
    Load a 2D scalar field from the grid file
  \item \texttt{{\bf grid\_load3d}(Field3D, ``name'')} \\
    Load a 3D scalar field from the grid file
  \item \texttt{{\bf invert\_laplace}(Field input, Field output, flags, Field2D *A)}
  \item \texttt{Field = {\bf invert\_parderiv}(Field2D|BoutReal A, Field2D|BoutReal B, Field3D r)} \\
    Inverts an equation  \code{A*x + B*Grad2\_par2(x) = r}
  \item \texttt{Field = {\bf Laplacian}(Field)}
  \item \texttt{Field3D = {\bf low\_pass}(Field3D, max\_modenr)}
  \item \texttt{BoutReal = {\bf max}(Field)}
  \item \texttt{BoutReal = {\bf min}(Field)}
  \item \texttt{{\bf msg\_stack.pop}( |int)} \\
    Remove a message from the top of the stack. If a message ID is passed,
    removes all messages back to that point.
  \item \texttt{int = {\bf msg\_stack.push}(``format'', ...)} \\
    Put a message onto the stack. Works like \code{printf} (and \code{output.write}).
  \item \texttt{{\bf options.get}(``name'', variable, default)} \\
    Get an integer, real or boolean value from the options file.
    If not in the file, the default value is used. The value
    used is printed to log file.
  \item \texttt{{\bf options.setSection}(``name'')}
    Set the section name in the input file
  \item \texttt{{\bf output} $< <$ values} \\
    Behaves like cout for stream output
  \item \texttt{{\bf output.write}(``format'', ...)}  \\
    Behaves like printf for formatted output
  \item \texttt{(Communicator).{\bf{receive}}()} \\
    Receive data from other processors. Must be preceded by a \code{send} call.
  \item \texttt{(Communicator).{\bf{run}}()} \\
    Sends and receives data.
  \item \texttt{(Communicator).{\bf{send}}()} \\
    Sends data to other processors (and posts receives). This must be followed
    by a call to \code{receive()} before calling send again, or adding new variables.
  \item \texttt{(Field3D)\bf{.setLocation}(CELL\_LOC)}
  \item \texttt{(Field3D)\bf{.ShiftZ}(bool)}
  \item \texttt{Field = {\bf{sin}}(Field)}
  \item \texttt{Field = {\bf{sinh}}(Field)}
  \item \texttt{{\bf solver.setPrecon}(PhysicsPrecon)} \\
    Set a preconditioner function
  \item \texttt{Field = \bf{sqrt}(Field)}
  \item \texttt{Field = {\bf tan}(Field)}
  \item \texttt{Field = {\bf tanh}(Field)}
  \item \texttt{Field = {\bf V\_dot\_Grad}(Vector v, Field f)} \\
    Calculates an advection term $\mathbf{v}\cdot\nabla f$
  \item \texttt{Vector = {\bf V\_dot\_Grad}(Vector v, Vector u)} \\
    Advection term $\mathbf{v}\cdot\nabla\mathbf{u}$
  \item \texttt{Field = {\bf Vpar\_Grad\_par}(Field v, Field f)}
  \item \texttt{Field3D = {\bf where}(Field2D test, Field|BoutReal gt0, Field|BoutReal lt0)} \\
    Chooses between two values, depending on sign of \code{test}.
\end{itemize}

\section{IDL routines}
\label{apx:idl_routines}

List of IDL routines available in idllib. There are broadly three categories
of routine:
\begin{itemize}
\item Completely general routines which could be useful outside BOUT++ work
  \begin{itemize}
  \item Data plotting and animation: {\bf contour2} and {\bf showdata}
  \item File reading and writing: {\bf file\_open}, {\bf file\_read} etc.
  \item User input and output: {\bf get\_float}, {\bf get\_integer}, {\bf get\_yesno} and {\bf str}
  \item FFT routines for integrating, differentiating and filtering: {\bf fft\_integrate}, {\bf fft\_deriv}, {\bf fft\_filter} 
  \end{itemize}
\item Routines for BOUT++, but not specific to any application
  \begin{itemize}
  \item Modifying restart files: {\bf expand\_restarts}, {\bf scale\_restarts} and {\bf split\_restarts}
  \item Processing 3D variables for input grid: {\bf bout3dvar}
  \end{itemize}
\item Routines specifically for tokamak simulations
  \begin{itemize}
  \item Reading A- and G-EQDSK format files into IDL: {\bf read\_aeqdsk} and {\bf read\_neqdsk}
  \item Plotting results: {\bf polslice}, {\bf plotpolslice}
  \end{itemize}
\end{itemize}

Here the format is

{\bf name}, arguments, [optional arguments]

\begin{itemize}
\item var = {\bf bout3dvar} ( var ) \\
  Converts 3D variables to and from BOUT++'s Fourier representation
  which is used for input grids. By default converts from [x,y,z] to [x,y,f]
  \begin{itemize}
  \item {\bf /reverse}  Convert from [x,y,f] to [x,y,z]
  \item {\bf nf}=nf Set number of frequencies in the result
  \item {\bf nz}=nz When using /reverse, set number of Z points in the result
  \end{itemize}
\item var = {\bf collect}() \\
  Read in data from a set of BOUT++ dump files
  \begin{itemize}
    \item {\bf var} = ``name of variable''
    \item {\bf path} = ``path/to/variable/''
    \item {\bf xind}, {\bf yind}, {\bf zind}, {\bf tind}  = [min, max] index pairs
    \item {\bf t\_array} = Output 1D array of times
  \end{itemize}
\item {\bf contour2}, data [, x, y] \\
  This is a replacement for the IDL contour which includes
  a scale color bar.
  \begin{itemize}
  \item {\bf data} can be either 2D (x,y) or 3D (x,y,t). If data
    is 3D then the color is scaled to the entire range.
  \item {\bf x} is an optional 2D (x,y) array of X coordinates
  \item {\bf y} is an optional 2D (x,y) array of Y coordinates
  \item {\bf t}=t is a time index for 3D data
  \item {\bf nlev}=nlev
  \item {\bf centre}=centre  Make zero the middle of the color
    range (white if redblue)
  \item {\bf redblue}=redblue  Use a blue-white-red color scheme
  \item {\bf revcolor}=revcolor  Reverse color scheme
  \end{itemize}
\item {\bf expand\_restarts}, newz \\
  Increases the number of Z points in restart files. Together with
  scale\_restarts and split\_restarts, this makes it easier to modify a
  linear simulation as a start for nonlinear runs.
  \begin{itemize}
  \item {\bf newz} is the new value of NZ
  \item {\bf path}=path      Input path
  \item {\bf output}=output  Output path
  \item {\bf format}=format  File extension of output
  \end{itemize}
\item result = {\bf fft\_deriv} ( var1d ) \\
  Calculates the derivative of a variable on a periodic domain.
\item result = {\bf fft\_filter} (var, nf)
  Fourier filter a variable on a periodic domain. Arguments are a
  1D variable and the number of Fourier components to keep
\item result = {\bf fft\_integrate} ( var1d )
  Integrates a variable on a periodic domain.
  \begin{itemize}
  \item {\bf loop}=loop  The loop integral is returned in this variable
  \end{itemize}
\item {\bf file\_close}, handle \\
  Close a file opened using file\_open()
\item list = {\bf file\_list} ( handle ) \\
  Return a list of variable names in the file
\item integer = {\bf file\_ndims} ( handle , ``variable'' ) \\
  Get the number of dimensions of a variable
\item handle = {\bf file\_open} ( ``file'' ) \\
  Open a PDB or NetCDF file. File type is inferred from file name
  \begin{itemize}
    \item {\bf /write}  Open file for writing (default is read only)
    \item {\bf /create} Create a new file, over-writing if already exists
  \end{itemize}
\item var = {\bf file\_read} ( handle, ``variable'' )
  \begin{itemize}
    \item {\bf inds} = [xmin, xmax, ymin, ymax, ... ]
  \end{itemize}
\item float = {\bf get\_float} ( ``prompt'' ) \\
  Ask the user for a float, using the given prompt
\item integer = {\bf get\_integer} ( ``prompt'' ) \\
  Ask the user for an integer
\item integer = {\bf get\_yesno} ( ``prompt'' ) \\
  Ask for a yes (1) or no (0) answer
\item result = {\bf gmres} ( x0, operator, b ) \\
  General Minimal Residual (GMRES)
   \begin{itemize}
    \item {\bf x0} is the starting guess at the solution
    \item {\bf operator} 
    \item {\bf b}
   \end{itemize}
   Optional arguments
   \begin{itemize}
   \item {\bf restart}=restart 
   \item {\bf max\_iter}=max\_iter
   \item {\bf tol}=tol
   \item {\bf stats}=stats
   \item {\bf show}=show
   \item {\bf output}=output
   \end{itemize}
\item result = {\bf int\_func} ( [x,] f ) \\
  Integrate a function, always using the maximum
  number of grid-points possible for highest accuracy
\item bool = {\bf is\_pow2} ( value ) \\
  Returns 1 (true) if the given number is a power of 2, 0 (false) otherwise
\item {\bf plotpolslice}, var3d, grid \\
  Takes a slice through a field-aligned tokamak domain, showing a poloidal cross-section.
  \begin{itemize}
  \item {\bf var3d} is a 3D (x,y,z) variable to plot. Needs all of the points to work properly.
  \item {\bf grid} is a structure from importing a grid file
  \end{itemize}
  Optional arguments:
  \begin{itemize}
  \item {\bf period}=period
  \item {\bf zangle}=zangle
  \item {\bf nlev}=nlev
  \item {\bf yr}=yr
  \item {\bf profile}=profile
  \item {\bf output}=output
  \item {\bf lines}=lines
  \item {\bf linecol}=linecol
  \item {\bf filter}=filter
  \end{itemize}
\item {\bf polslice}, data, gridfile \\
  Plots a 2D poloidal contour for single or double-null configurations, including color bar.
  \begin{itemize}
    \item {\bf xstart}=xstart  X index where the data begins. Useful if only part of the domain has been collected
    \item {\bf ystart}=ystart  Y index where data begins
  \end{itemize}
\item struct = {\bf read\_aeqdsk} ( "filename" ) \\
  Reads an A-EQDSK file. Format is specified here: 
  \url{https://fusion.gat.com/THEORY/efit/a_eqdsk.html}
\item struct = {\bf read\_neqdsk} ( "filename" ) \\
  Reads in an 'neqdsk' or G-EQDSK formatted tokamak equilibrium file.
  Format of G-EQDSK file is specified here:
  \url{https://fusion.gat.com/THEORY/efit/g_eqdsk.html}
\item stringarray = {\bf regex\_extract} ( line, pattern ) \\
  Extract all matches to Regular Expression pattern contained in line. 
  Useful for extracting numbers from FORTRAN-formatted text files.
  \begin{itemize}
  \item {\bf line} Input string
  \item {\bf pattern} Regular expression pattern to match 
  \item {\bf nmatch}=nmatch
  \end{itemize}
\item var = {\bf reverse\_inds} ( var ) \\
  Reverse array indices e.g. \code{arr[t,z,y,x] -> arr[x,y,z,t]}. Works on up to 5 dimensional variables
\item {\bf safe\_colors} \\
  Sets the color table to useful values for plotting.
  \begin{itemize}
  \item {\bf /first}   Sets the first 10 colors to specific values, otherwise sets last 7
  \end{itemize}
\item {\bf scale\_restarts}, factor
  \begin{itemize}
  \item {\bf path}=path  Path to the restart files (default is current directory '.')
  \item {\bf format}=format  Specify what the file format is, otherwise goes on the file name
  \end{itemize}
\item {\bf showdata}, data \\
  Display animations of 1D,2D and 3D data. Defaults:
  \begin{itemize}
  \item 2D data   Animate a line plot
  \item 3D data   Animate a surface plot
  \item 4D data   Animate a poloidal cross-section (tokamaks only)
  \end{itemize}
  Optional arguments:
  \begin{itemize}
  \item {\bf /addsym}   For 2D data (1D plots), add symbols to mark data points
  \item {\bf az}=angle       Rotate surface plots
  \item {\bf /bw} Make contour plots greyscale
  \item {\bf chars}=size character size
  \item {\bf /contour}  For 3D input, show color contour plot
  \item {\bf delay}=time     Time delay between plots (default 0.2 seconds)
  \item {\bf /noscale}       By default, all plots are on the same scale. 
    This changes the scale for each plot's range
  \item {\bf profile}=array  Background profile. Data is 3D: profile is 1D (X). 
    Data is 4D -> profile is 2D (X,Y)
  \item {\bf yr}=[min,max]   Y range
  \end{itemize}
\item result = {\bf sign} ( var ) \\
  This returns +1 if the variable is $> 0$, -1 otherwise
\item {\bf spectrum}
\item {\bf split\_restarts}, [nxpe], nype \\
  split restart files between a different number of processors
  \begin{itemize}
  \item {\bf nxpe} is an optional argument giving the number of
    processors in the X direction
  \item {\bf nype} is the number of processors in the Y direction
  \item {\bf path}=path      Input path
  \item {\bf output}=output  Output path
  \item {\bf format}=format  File extension of output
  \end{itemize}
\item string = {\bf str} ( value ) \\
  Convert a value to a string with whitespace trimmed. Arrays are
  converted to a comma-separated list in brackets.
\item result = {\bf zfamp} ( var4d )\\
  Given a 4D variable [x,y,z,t], returns the Fourier amplitudes in [x,y,f,t]
\item var = {\bf zshift} ( var, shift ) \\
  Shifts a variable in the Z direction, useful for mapping between field-aligned and orthogonal
  coordinates. 
  \begin{itemize}
  \item {\bf period}=period  How many domains fit in $2\pi$. Default is 1 (full torus)
  \end{itemize}
\end{itemize}

\section{Python routines (alphabetical)}
\label{apx:py_routines}

\begin{itemize}
\item Datafile()  Constructor
\end{itemize}

\printindex

\end{document}
